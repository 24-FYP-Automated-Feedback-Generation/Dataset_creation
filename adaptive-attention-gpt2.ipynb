{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10048507,"sourceType":"datasetVersion","datasetId":6190931}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"4a407760-fece-43e3-83f0-258295fc1289","_cell_guid":"bce0f255-1c74-4800-874e-e0a0d21c6c65","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:22:13.851634Z","iopub.execute_input":"2024-12-16T17:22:13.851976Z","iopub.status.idle":"2024-12-16T17:22:13.858910Z","shell.execute_reply.started":"2024-12-16T17:22:13.851940Z","shell.execute_reply":"2024-12-16T17:22:13.858036Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/modified-dataset/modified_dataset.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Import libraries.","metadata":{"_uuid":"c86ca820-e375-440f-8ad6-e6b4bbaff0a0","_cell_guid":"c09d0da7-12e6-455d-b8b9-bd6197e9c237","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"pip install transformers torch","metadata":{"_uuid":"52d993c3-bed9-4d4a-a6e5-50d60b95de42","_cell_guid":"505ee843-a99c-438e-b45a-2f85e8286c43","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:22:17.154144Z","iopub.execute_input":"2024-12-16T17:22:17.154474Z","iopub.status.idle":"2024-12-16T17:22:26.440735Z","shell.execute_reply.started":"2024-12-16T17:22:17.154444Z","shell.execute_reply":"2024-12-16T17:22:26.439701Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoModel, AutoTokenizer, GPT2Model","metadata":{"_uuid":"54c0ca1b-e865-4607-b7ea-f6b5f55e1ae3","_cell_guid":"82066fba-f007-4733-9e9f-6890e813fc35","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:22:26.442426Z","iopub.execute_input":"2024-12-16T17:22:26.442730Z","iopub.status.idle":"2024-12-16T17:22:42.447471Z","shell.execute_reply.started":"2024-12-16T17:22:26.442700Z","shell.execute_reply":"2024-12-16T17:22:42.446785Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"abd85071-a8e0-4085-8807-5f20f02b96bb","_cell_guid":"6d660b0a-145b-40be-a319-a80a63ae5a8e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:22:42.448320Z","iopub.execute_input":"2024-12-16T17:22:42.448762Z","iopub.status.idle":"2024-12-16T17:22:42.510405Z","shell.execute_reply.started":"2024-12-16T17:22:42.448734Z","shell.execute_reply":"2024-12-16T17:22:42.509523Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device","metadata":{"_uuid":"476e590d-eb56-4ce5-b4a2-4180bd480487","_cell_guid":"a7335a65-4e7d-413e-b0f7-b564dc5f7c78","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:22:42.513091Z","iopub.execute_input":"2024-12-16T17:22:42.513516Z","iopub.status.idle":"2024-12-16T17:22:42.531233Z","shell.execute_reply.started":"2024-12-16T17:22:42.513468Z","shell.execute_reply":"2024-12-16T17:22:42.530275Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Models initialization and tokenizations","metadata":{"_uuid":"396fe6e6-fef5-4616-9ce8-259b587e3ecd","_cell_guid":"d7275979-13ad-4bfe-aaa2-6b9bd8294f2e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model_name_encoder = \"bert-base-uncased\"","metadata":{"_uuid":"690570be-1c5b-4a40-81ab-d6afaed6ca93","_cell_guid":"2cd89c4a-ee79-4cfb-a64e-9412a9120f60","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:22:42.532134Z","iopub.execute_input":"2024-12-16T17:22:42.532353Z","iopub.status.idle":"2024-12-16T17:22:42.542460Z","shell.execute_reply.started":"2024-12-16T17:22:42.532331Z","shell.execute_reply":"2024-12-16T17:22:42.541602Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"context_encoder = AutoModel.from_pretrained(model_name_encoder)\n#this same encoder will be used as the persona encoder but with a linear projection of 16->768","metadata":{"_uuid":"e32d4fd7-1858-407a-915d-3d3cd08ef64a","_cell_guid":"41d207bd-4d33-45f7-8d80-a7b24aa5edf2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:22:42.543345Z","iopub.execute_input":"2024-12-16T17:22:42.543673Z","iopub.status.idle":"2024-12-16T17:22:45.209095Z","shell.execute_reply.started":"2024-12-16T17:22:42.543628Z","shell.execute_reply":"2024-12-16T17:22:45.208409Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc902432571a4d9da945d0dae44a98a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"319602cc0adb4bcaaa58cdcd16a4a5df"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"decoder = GPT2Model.from_pretrained(\"gpt2\")","metadata":{"_uuid":"f1b6665f-5aaf-419d-8cb6-cc07900f9ec8","_cell_guid":"70a7e943-ee33-4ca5-8a75-8cf1a6c20387","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:22:45.210097Z","iopub.execute_input":"2024-12-16T17:22:45.210335Z","iopub.status.idle":"2024-12-16T17:22:48.229338Z","shell.execute_reply.started":"2024-12-16T17:22:45.210311Z","shell.execute_reply":"2024-12-16T17:22:48.228549Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"596a5372c9a9421295d283d6661952ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f129fb4fa4d04643851aa99a03589c2a"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"decoder.resize_token_embeddings(decoder.config.vocab_size)","metadata":{"_uuid":"f5419cd6-38c2-4241-a363-61cbbef96ac2","_cell_guid":"da3cb94a-d8dc-4fca-a856-ec8e07d977ce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:22:48.230547Z","iopub.execute_input":"2024-12-16T17:22:48.230903Z","iopub.status.idle":"2024-12-16T17:22:48.236923Z","shell.execute_reply.started":"2024-12-16T17:22:48.230865Z","shell.execute_reply":"2024-12-16T17:22:48.236081Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Embedding(50257, 768)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Tokenizer\ntokenizer_encoder = AutoTokenizer.from_pretrained(model_name_encoder)","metadata":{"_uuid":"71021b85-ce78-4ad0-b401-7d42499e57e4","_cell_guid":"00345d6d-d9b9-4a69-91b9-acbcbae9b7bc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:22:48.237775Z","iopub.execute_input":"2024-12-16T17:22:48.238111Z","iopub.status.idle":"2024-12-16T17:22:49.520033Z","shell.execute_reply.started":"2024-12-16T17:22:48.238076Z","shell.execute_reply":"2024-12-16T17:22:49.519359Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1157c00b33a64139b8f86223d4414f5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ead6953eaa24eb09328d741d42ac812"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ffe228d7ab4362a80b920fa1177eca"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"tokenizer_decoder = AutoTokenizer.from_pretrained(\"gpt2\")","metadata":{"_uuid":"2866246d-8def-4bca-9dfc-26c760d8c276","_cell_guid":"4f2fef67-ea28-4d83-bf4f-59f65d43ed79","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:22:49.522905Z","iopub.execute_input":"2024-12-16T17:22:49.523208Z","iopub.status.idle":"2024-12-16T17:22:51.278272Z","shell.execute_reply.started":"2024-12-16T17:22:49.523183Z","shell.execute_reply":"2024-12-16T17:22:51.277030Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f303027f3ae740d6bf94e8a6c8b4b2c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba4dba82ec542a696e85ea8d10f3eb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f565854fb54742aeb30cc47463b552c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f675dd343a264c9c854b6c9934ad8d7c"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Set padding token to eos_token for GPT-2\ntokenizer_decoder.pad_token = tokenizer_decoder.eos_token","metadata":{"_uuid":"2349b441-e524-4069-b1c4-477ede9e4681","_cell_guid":"04eed121-e7f5-4cd3-afc9-75fb658dcd5f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:22:51.279442Z","iopub.execute_input":"2024-12-16T17:22:51.279731Z","iopub.status.idle":"2024-12-16T17:22:51.283691Z","shell.execute_reply.started":"2024-12-16T17:22:51.279703Z","shell.execute_reply":"2024-12-16T17:22:51.282866Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Learnable projection layer for metacognitive profile\nprofile_projection = nn.Linear(16, context_encoder.config.hidden_size)","metadata":{"_uuid":"3516c118-4152-4f72-aec3-6e00babf7594","_cell_guid":"d3ff658b-dd0c-4cb6-8eb9-d03179b63c7a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:22:51.284779Z","iopub.execute_input":"2024-12-16T17:22:51.285217Z","iopub.status.idle":"2024-12-16T17:22:51.575051Z","shell.execute_reply.started":"2024-12-16T17:22:51.285165Z","shell.execute_reply":"2024-12-16T17:22:51.574240Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Defining the PAA layers and Model","metadata":{"_uuid":"5da23592-dc68-471c-b4c0-440668a52ead","_cell_guid":"6e822047-2050-4fcd-9a16-ac15230bec27","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class PAALayer(nn.Module):\n    def __init__(self, hidden_size):\n        super(PAALayer, self).__init__()\n        self.cross_attn = nn.MultiheadAttention(hidden_size, num_heads=8)\n        self.sigmoid = nn.Sigmoid()\n        self.linear = nn.Linear(hidden_size * 2, hidden_size)\n\n    def forward(self, persona_hidden, context_hidden, decoder_hidden, tau):\n        c1, _ = self.cross_attn(decoder_hidden, persona_hidden, persona_hidden)\n        c2, _ = self.cross_attn(decoder_hidden, context_hidden, context_hidden)\n        \n        # Adaptive weight calculation\n        w1 = self.sigmoid(self.linear(torch.cat((c1, decoder_hidden), dim=-1)))\n        w2 = 1 - w1\n\n        # Mask creation\n        m1 = torch.where(w1 > tau, 0, 1)\n        m2 = torch.where(w1 < 1 - tau, 0, 1)\n\n        # Weighted summation with masks\n        paa_output = w1 * m1 * c1 + w2 * m2 * c2 + decoder_hidden\n        return paa_output","metadata":{"_uuid":"b23db8c0-6492-4b62-bb1b-308e58de78a8","_cell_guid":"069d098b-564e-47fb-bd89-f91b70ae2944","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:22:51.576136Z","iopub.execute_input":"2024-12-16T17:22:51.576448Z","iopub.status.idle":"2024-12-16T17:22:51.583103Z","shell.execute_reply.started":"2024-12-16T17:22:51.576422Z","shell.execute_reply":"2024-12-16T17:22:51.582369Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PAA_Model(nn.Module):\n    def __init__(self, context_encoder, decoder, profile_projection, paa_layer):\n        super(PAA_Model, self).__init__()\n        self.context_encoder = context_encoder\n        self.decoder = decoder\n        self.profile_projection = profile_projection\n        self.paa_layer = paa_layer\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, context_tokens, target_tokens, profile_vector, tau):\n        # Encode context\n        context_hidden = self.context_encoder(**context_tokens).last_hidden_state\n        print(f\"context_hidden shape: {context_hidden.shape}\")\n\n        # Project metacognitive profile\n        projected_profile = self.profile_projection(profile_vector).unsqueeze(1)\n        print(f\"projected profile shape: {projected_profile.shape}\")\n\n        # Expand persona representation\n        persona_hidden = projected_profile.expand(-1, context_hidden.size(1), -1)\n        print(f\"persona_hidden shape: {persona_hidden.shape}\")\n\n        # Resize or pad persona_hidden and context_hidden to match decoder_hidden length\n        target_length = target_tokens['input_ids'].shape[1]  # Match target sequence length (e.g., decoder_hidden length)\n        context_hidden_resized = self.resize_sequence(context_hidden, target_length)\n        persona_hidden_resized = self.resize_sequence(persona_hidden, target_length)\n\n        print(f\"Resized context_hidden shape: {context_hidden_resized.shape}\")\n        print(f\"Resized persona_hidden shape: {persona_hidden_resized.shape}\")\n\n        # Decoder's output\n        decoder_hidden = self.decoder(**target_tokens).last_hidden_state\n        print(f\"decoder_hidden shape: {decoder_hidden.shape}\")\n\n        # Apply PAA\n        paa_output = self.paa_layer(persona_hidden_resized, context_hidden_resized, decoder_hidden, tau)\n\n        # No softmax here, CrossEntropyLoss expects raw logits\n        logits = paa_output\n        target = target_tokens['input_ids'][:, 1:].contiguous().view(-1)  # Flatten target tokens\n        print(f\"logits shape: {logits.shape}\")\n        print(f\"target shape: {target.shape}\")\n\n        # Ensure logits and target have matching batch size\n        logits = logits.view(-1, logits.size(-1))  # Shape: (batch_size * seq_len, vocab_size)\n\n        assert logits.size(0) == target.size(0), f\"Batch size mismatch: logits batch size {logits.size(0)} vs target batch size {target.size(0)}\"\n\n        # Calculate loss\n        loss = self.loss_fn(logits, target)\n        return loss\n\n    def resize_sequence(self, tensor, target_length):\n        \"\"\"\n        Resize tensor sequence to the target length using padding or interpolation.\n        This method can be adjusted to use either padding or interpolation as per the requirement.\n        \"\"\"\n        current_length = tensor.size(1)\n        if current_length < target_length:\n            # Padding case\n            padding_length = target_length - current_length\n            return F.pad(tensor, (0, 0, 0, padding_length), \"constant\", 0)\n        elif current_length > target_length:\n            # Truncation case\n            return tensor[:, :target_length, :]\n        else:\n            return tensor  # No change if lengths match\n","metadata":{"_uuid":"38c60efb-a170-4407-9358-c4f0672f0730","_cell_guid":"bf4ca38d-3ddf-495f-93c0-451889c297d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:36:57.976016Z","iopub.execute_input":"2024-12-16T17:36:57.976617Z","iopub.status.idle":"2024-12-16T17:36:57.987142Z","shell.execute_reply.started":"2024-12-16T17:36:57.976579Z","shell.execute_reply":"2024-12-16T17:36:57.986171Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# Model and optimizer","metadata":{"_uuid":"a65a11be-0a5d-436c-bada-99dfa399bf9e","_cell_guid":"88f012aa-6217-45b6-907c-f47691a5c9cb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"paa_layer = PAALayer(context_encoder.config.hidden_size)","metadata":{"_uuid":"95f28036-1e0a-41b9-a776-deaabb4f0877","_cell_guid":"3a29e610-71e5-4402-8a5a-d0a95556a1ce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:23:06.794211Z","iopub.execute_input":"2024-12-16T17:23:06.794555Z","iopub.status.idle":"2024-12-16T17:23:06.824911Z","shell.execute_reply.started":"2024-12-16T17:23:06.794525Z","shell.execute_reply":"2024-12-16T17:23:06.824227Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model = PAA_Model(context_encoder, decoder, profile_projection, paa_layer).to(device)","metadata":{"_uuid":"c586fdef-8f52-4691-96c2-2f4d1781fa57","_cell_guid":"35577727-423a-4058-9dcd-b1678cabe6d4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:37:01.063844Z","iopub.execute_input":"2024-12-16T17:37:01.064520Z","iopub.status.idle":"2024-12-16T17:37:01.074063Z","shell.execute_reply.started":"2024-12-16T17:37:01.064487Z","shell.execute_reply":"2024-12-16T17:37:01.073318Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"optimizer = Adam(model.parameters(), lr=5e-5)","metadata":{"_uuid":"dda16650-0ad9-419e-9079-3655382ce3df","_cell_guid":"e9030db2-ec81-4254-8c73-2dc9a1b05047","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:23:11.639267Z","iopub.execute_input":"2024-12-16T17:23:11.639928Z","iopub.status.idle":"2024-12-16T17:23:11.645445Z","shell.execute_reply.started":"2024-12-16T17:23:11.639893Z","shell.execute_reply":"2024-12-16T17:23:11.644522Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{"_uuid":"d4c7d311-5619-4ee4-9ebd-1fed8add3c77","_cell_guid":"0992d609-3994-4dc7-9d34-addfa6b3f52c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"file_path = \"/kaggle/input/modified-dataset/modified_dataset.csv\"\ndf = pd.read_csv(file_path)","metadata":{"_uuid":"0f0e332c-7410-47ef-ab17-fb4d76318b9f","_cell_guid":"57dd7de2-47b0-48a3-a353-3f2834ff6fa3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:23:14.314296Z","iopub.execute_input":"2024-12-16T17:23:14.314653Z","iopub.status.idle":"2024-12-16T17:23:14.360636Z","shell.execute_reply.started":"2024-12-16T17:23:14.314618Z","shell.execute_reply":"2024-12-16T17:23:14.359956Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"df.head(10)","metadata":{"_uuid":"aabe0e36-21e0-4cb7-83e7-7288c442cd35","_cell_guid":"5188d244-16c3-4497-998a-b5b736a2a798","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:23:15.674699Z","iopub.execute_input":"2024-12-16T17:23:15.675048Z","iopub.status.idle":"2024-12-16T17:23:15.692607Z","shell.execute_reply.started":"2024-12-16T17:23:15.675019Z","shell.execute_reply":"2024-12-16T17:23:15.691736Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                         description  \\\n0  Create a Python program that performs the foll...   \n1  Create a Python program that accomplishes the ...   \n2  Create a Python program that accomplishes the ...   \n3  Create a Python program that accomplishes the ...   \n4  Create a Python program that accomplishes the ...   \n5  Create a Python program that accomplishes the ...   \n6  Create a Python program that accomplishes the ...   \n7  Create a Python program that accomplishes the ...   \n8  Create a program that carries out the followin...   \n9  Your task is to create a function named 'count...   \n\n                                        student_code  \\\n0  \"\"\" store the final answer in a variable named...   \n1  \"\"\" store the final answer in a variable named...   \n2  \"\"\" store the final answer in a variable named...   \n3  x=eval(input(\"Enter your age:\"))\\ny=str(input(...   \n4  n = str(input(\"Enter your name:\"))\\na = str(in...   \n5  \"\"\" store your answer in a variable named resu...   \n6  name = input(\"Enter your name: \")\\nage = eval(...   \n7  \"\"\" store your answer in a variable named resu...   \n8  \"\"\" store your answer in a variable named resu...   \n9  def count_substring(string):\\n    ans = 0\\n   ...   \n\n                                            feedback  \\\n0  [\\n    {\\n    'line_number': 2,\\n    'feedback...   \n1  [\\n    {\\n        'line_number': 4,\\n        '...   \n2  [\\n    {\\n        'line_number': 2,\\n        '...   \n3  [\\n    {\\n        'line_number': 1,\\n        '...   \n4  [\\n    {\\n        'line_number': 3,\\n        '...   \n5  [\\n    {\\n    'line_number':  4,\\n    'feedbac...   \n6  [\\n    {\\n        'line_number': 2,\\n        '...   \n7  [\\n    {\\n    \"line_number\": 2,\\n    \"feedback...   \n8  [\\n    {\\n        'line_number': 1,\\n        '...   \n9  [\\n    {\\n    \"line_number\":  4,\\n    \"feedbac...   \n\n                              metacognitive_feedback  \\\n0  It appears that you are almost on the right tr...   \n1  To improve your solution and better align with...   \n2  Based on your approach to the problem, it seem...   \n3  Based on your approach, it seems like you ofte...   \n4  **Metacognitive Feedback**:\\n\\nYou've made a g...   \n5  ### Metacognitive Feedback:\\n\\nYou have demons...   \n6  ### Metacognitive Feedback:\\n\\nYou have a stro...   \n7  To improve your approach to solving this progr...   \n8  You have made a good start, but there are seve...   \n9  Certainly! Let's break down the feedback to en...   \n\n                              metacognitive_profile  \n0  [2, 1, 3, 3, 2, 3, 2, 1, 3, 1, 1, 3, 2, 1, 2, 1]  \n1  [3, 1, 2, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 2, 2, 1]  \n2  [2, 1, 1, 2, 2, 3, 3, 1, 3, 3, 2, 1, 2, 1, 2, 2]  \n3  [1, 3, 1, 3, 1, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2]  \n4  [3, 1, 3, 3, 2, 1, 3, 3, 3, 1, 2, 3, 2, 1, 1, 3]  \n5  [3, 2, 3, 3, 2, 3, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2]  \n6  [3, 2, 1, 1, 3, 3, 1, 1, 3, 2, 2, 2, 1, 1, 2, 3]  \n7  [2, 1, 1, 2, 1, 3, 3, 1, 3, 2, 1, 3, 2, 1, 2, 3]  \n8  [2, 1, 3, 1, 3, 1, 1, 2, 3, 1, 3, 1, 2, 2, 2, 3]  \n9  [1, 1, 3, 2, 2, 1, 3, 2, 2, 3, 1, 1, 1, 2, 2, 2]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n      <th>student_code</th>\n      <th>feedback</th>\n      <th>metacognitive_feedback</th>\n      <th>metacognitive_profile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Create a Python program that performs the foll...</td>\n      <td>\"\"\" store the final answer in a variable named...</td>\n      <td>[\\n    {\\n    'line_number': 2,\\n    'feedback...</td>\n      <td>It appears that you are almost on the right tr...</td>\n      <td>[2, 1, 3, 3, 2, 3, 2, 1, 3, 1, 1, 3, 2, 1, 2, 1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>\"\"\" store the final answer in a variable named...</td>\n      <td>[\\n    {\\n        'line_number': 4,\\n        '...</td>\n      <td>To improve your solution and better align with...</td>\n      <td>[3, 1, 2, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 2, 2, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>\"\"\" store the final answer in a variable named...</td>\n      <td>[\\n    {\\n        'line_number': 2,\\n        '...</td>\n      <td>Based on your approach to the problem, it seem...</td>\n      <td>[2, 1, 1, 2, 2, 3, 3, 1, 3, 3, 2, 1, 2, 1, 2, 2]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>x=eval(input(\"Enter your age:\"))\\ny=str(input(...</td>\n      <td>[\\n    {\\n        'line_number': 1,\\n        '...</td>\n      <td>Based on your approach, it seems like you ofte...</td>\n      <td>[1, 3, 1, 3, 1, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>n = str(input(\"Enter your name:\"))\\na = str(in...</td>\n      <td>[\\n    {\\n        'line_number': 3,\\n        '...</td>\n      <td>**Metacognitive Feedback**:\\n\\nYou've made a g...</td>\n      <td>[3, 1, 3, 3, 2, 1, 3, 3, 3, 1, 2, 3, 2, 1, 1, 3]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>\"\"\" store your answer in a variable named resu...</td>\n      <td>[\\n    {\\n    'line_number':  4,\\n    'feedbac...</td>\n      <td>### Metacognitive Feedback:\\n\\nYou have demons...</td>\n      <td>[3, 2, 3, 3, 2, 3, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>name = input(\"Enter your name: \")\\nage = eval(...</td>\n      <td>[\\n    {\\n        'line_number': 2,\\n        '...</td>\n      <td>### Metacognitive Feedback:\\n\\nYou have a stro...</td>\n      <td>[3, 2, 1, 1, 3, 3, 1, 1, 3, 2, 2, 2, 1, 1, 2, 3]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>\"\"\" store your answer in a variable named resu...</td>\n      <td>[\\n    {\\n    \"line_number\": 2,\\n    \"feedback...</td>\n      <td>To improve your approach to solving this progr...</td>\n      <td>[2, 1, 1, 2, 1, 3, 3, 1, 3, 2, 1, 3, 2, 1, 2, 3]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Create a program that carries out the followin...</td>\n      <td>\"\"\" store your answer in a variable named resu...</td>\n      <td>[\\n    {\\n        'line_number': 1,\\n        '...</td>\n      <td>You have made a good start, but there are seve...</td>\n      <td>[2, 1, 3, 1, 3, 1, 1, 2, 3, 1, 3, 1, 2, 2, 2, 3]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Your task is to create a function named 'count...</td>\n      <td>def count_substring(string):\\n    ans = 0\\n   ...</td>\n      <td>[\\n    {\\n    \"line_number\":  4,\\n    \"feedbac...</td>\n      <td>Certainly! Let's break down the feedback to en...</td>\n      <td>[1, 1, 3, 2, 2, 1, 3, 2, 2, 3, 1, 1, 1, 2, 2, 2]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"_uuid":"57da18f5-9229-4275-9174-02d9785d8ec8","_cell_guid":"fc5ba1f8-45ec-41fe-a4bb-135c78158664","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:15:00.737909Z","iopub.execute_input":"2024-12-16T17:15:00.738832Z","iopub.status.idle":"2024-12-16T17:15:00.743210Z","shell.execute_reply.started":"2024-12-16T17:15:00.738794Z","shell.execute_reply":"2024-12-16T17:15:00.742384Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import ast\ndef tokenize_data(df, tokenizer_encoder,tokenizer_decoder):\n    context_tokens = tokenizer_encoder(list(df['description']), padding=True, truncation=True, return_tensors=\"pt\")\n    target_tokens = tokenizer_decoder(list(df['metacognitive_feedback']), padding=True, truncation=True, return_tensors=\"pt\")\n    profile_vectors = torch.tensor([ast.literal_eval(profile) for profile in df['metacognitive_profile']], dtype=torch.float)\n    return context_tokens, target_tokens, profile_vectors","metadata":{"_uuid":"00ad70e8-6c81-4935-9250-c56fcaaca482","_cell_guid":"dfd22fe5-a704-40ab-ad2a-88c4a73a9176","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:23:18.974654Z","iopub.execute_input":"2024-12-16T17:23:18.975485Z","iopub.status.idle":"2024-12-16T17:23:18.980290Z","shell.execute_reply.started":"2024-12-16T17:23:18.975448Z","shell.execute_reply":"2024-12-16T17:23:18.979330Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"context_tokens, target_tokens, profile_vectors = tokenize_data(df, tokenizer_encoder,tokenizer_decoder)","metadata":{"_uuid":"09cc8670-66ac-43d3-a6eb-9143cdebd9ed","_cell_guid":"124b3a3d-a23e-4d05-8d66-b95c7d9f37d4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:23:20.394035Z","iopub.execute_input":"2024-12-16T17:23:20.394369Z","iopub.status.idle":"2024-12-16T17:23:20.803493Z","shell.execute_reply.started":"2024-12-16T17:23:20.394340Z","shell.execute_reply":"2024-12-16T17:23:20.802788Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"context_tokens = {key: value.to(device) for key, value in context_tokens.items()}\ntarget_tokens = {key: value.to(device) for key, value in target_tokens.items()}\nprofile_vectors = profile_vectors.to(device)","metadata":{"_uuid":"0a6037a1-8120-4f87-98d9-76ba55e7c9d0","_cell_guid":"8875658a-83ea-4932-bc7b-a90825f10a5f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:23:23.224172Z","iopub.execute_input":"2024-12-16T17:23:23.224507Z","iopub.status.idle":"2024-12-16T17:23:23.249533Z","shell.execute_reply.started":"2024-12-16T17:23:23.224478Z","shell.execute_reply":"2024-12-16T17:23:23.248857Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"context_tokens[0]","metadata":{"_uuid":"4fcb11c5-8eee-4047-9c58-3011b5635b74","_cell_guid":"9c24942c-2170-4cce-aee5-7673780005ab","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:16:16.465728Z","iopub.execute_input":"2024-12-16T17:16:16.466743Z","iopub.status.idle":"2024-12-16T17:16:16.496913Z","shell.execute_reply.started":"2024-12-16T17:16:16.466707Z","shell.execute_reply":"2024-12-16T17:16:16.495744Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcontext_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n","\u001b[0;31mKeyError\u001b[0m: 0"],"ename":"KeyError","evalue":"0","output_type":"error"}],"execution_count":30},{"cell_type":"code","source":"# Create DataLoader\ntrain_data = TensorDataset(context_tokens['input_ids'], target_tokens['input_ids'], profile_vectors)\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True)","metadata":{"_uuid":"1048c9d9-06ea-49e2-b642-c456d6c2655c","_cell_guid":"ec522685-b292-4e50-808a-32d0c5b880a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:23:26.854277Z","iopub.execute_input":"2024-12-16T17:23:26.854619Z","iopub.status.idle":"2024-12-16T17:23:26.859280Z","shell.execute_reply.started":"2024-12-16T17:23:26.854588Z","shell.execute_reply":"2024-12-16T17:23:26.858398Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Training loop","metadata":{"_uuid":"f71ce5bb-0a33-4482-b458-41e5bda33f4b","_cell_guid":"c6c808ad-3f19-40e8-baeb-05e6450d830b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model.train()\nnum_epochs = 5\ntau = 0.5\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for context_ids, target_ids, profile_vector in train_loader:\n        # Move tensors to the GPU (if available)\n        context_ids = context_ids.to(device)\n        target_ids = target_ids.to(device)\n        profile_vector = profile_vector.to(device)\n\n        optimizer.zero_grad()\n\n        # Prepare input tensors\n        context_tokens = {'input_ids': context_ids, 'attention_mask': context_ids != tokenizer_encoder.pad_token_id}\n        target_tokens = {'input_ids': target_ids, 'attention_mask': target_ids != tokenizer_decoder.pad_token_id}\n\n        # Forward pass through the model\n        loss = model(context_tokens, target_tokens, profile_vector, tau)\n\n        # Backpropagate and update the model\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")","metadata":{"_uuid":"04db2fd9-30da-45ef-a653-44d95a2d6a05","_cell_guid":"c154751d-5f67-47dc-91d8-fc6483a92e69","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:37:07.229578Z","iopub.execute_input":"2024-12-16T17:37:07.230319Z","iopub.status.idle":"2024-12-16T17:37:07.751473Z","shell.execute_reply.started":"2024-12-16T17:37:07.230282Z","shell.execute_reply":"2024-12-16T17:37:07.750007Z"}},"outputs":[{"name":"stdout","text":"context_hidden shape: torch.Size([8, 351, 768])\nprojected profile shape: torch.Size([8, 1, 768])\npersona_hidden shape: torch.Size([8, 351, 768])\nResized context_hidden shape: torch.Size([8, 743, 768])\nResized persona_hidden shape: torch.Size([8, 743, 768])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m target_tokens \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: target_ids, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: target_ids \u001b[38;5;241m!=\u001b[39m tokenizer_decoder\u001b[38;5;241m.\u001b[39mpad_token_id}\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Backpropagate and update the model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[27], line 36\u001b[0m, in \u001b[0;36mPAA_Model.forward\u001b[0;34m(self, context_tokens, target_tokens, profile_vector, tau)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResized persona_hidden shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersona_hidden_resized\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Decoder's output\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m decoder_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtarget_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_hidden shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_hidden\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Apply PAA\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1132\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1121\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1122\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         output_attentions,\n\u001b[1;32m   1130\u001b[0m     )\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:652\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    650\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    651\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 652\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    654\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:576\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 576\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    578\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.044715\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m3.0\u001b[39m))))\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 47.12 MiB is free. Process 2135 has 15.84 GiB memory in use. Of the allocated memory 15.09 GiB is allocated by PyTorch, and 474.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 47.12 MiB is free. Process 2135 has 15.84 GiB memory in use. Of the allocated memory 15.09 GiB is allocated by PyTorch, and 474.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"_uuid":"011133be-06db-4093-9092-c463080cdd2a","_cell_guid":"f03e3e51-e071-42cf-bd85-06570ed38368","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:07:38.511418Z","iopub.execute_input":"2024-12-16T17:07:38.512324Z","iopub.status.idle":"2024-12-16T17:07:38.517039Z","shell.execute_reply.started":"2024-12-16T17:07:38.512265Z","shell.execute_reply":"2024-12-16T17:07:38.516219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.memory_summary())","metadata":{"_uuid":"b26fc14c-f03c-4a29-ba67-b7863484b1e5","_cell_guid":"394b042d-57fd-4371-bd10-008765579754","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:11:28.010818Z","iopub.execute_input":"2024-12-16T17:11:28.011421Z","iopub.status.idle":"2024-12-16T17:11:28.016583Z","shell.execute_reply.started":"2024-12-16T17:11:28.011387Z","shell.execute_reply":"2024-12-16T17:11:28.015637Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"888646d8-5ba3-44e6-bd8e-778a3e8e354a","_cell_guid":"84d90ef4-9f39-476a-8075-4665e1d9a5d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}