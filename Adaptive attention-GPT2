{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10048507,"sourceType":"datasetVersion","datasetId":6190931}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"a253fd50-0c38-4ed2-95be-d8de722c76c0","_cell_guid":"879296a9-98ee-4c26-be52-e01b545a2867","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T14:38:39.413989Z","iopub.execute_input":"2024-12-17T14:38:39.414411Z","iopub.status.idle":"2024-12-17T14:38:39.760316Z","shell.execute_reply.started":"2024-12-17T14:38:39.414380Z","shell.execute_reply":"2024-12-17T14:38:39.759402Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/modified-dataset/modified_dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import libraries.","metadata":{"_uuid":"1ad07d1c-cedd-4a5a-b877-8725368c6600","_cell_guid":"74f900c3-cd95-40c7-bae9-4b6e72d883a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"pip install transformers torch","metadata":{"_uuid":"faf8192f-ccd7-4e28-ab6c-eac95a4dbbd4","_cell_guid":"19421aeb-0fda-47bb-8161-ce259118872e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T07:20:44.884724Z","iopub.execute_input":"2024-12-17T07:20:44.885626Z","iopub.status.idle":"2024-12-17T07:20:56.800661Z","shell.execute_reply.started":"2024-12-17T07:20:44.885591Z","shell.execute_reply":"2024-12-17T07:20:56.799568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoModel, AutoTokenizer, GPT2Model\nfrom transformers import BertModel, BertTokenizer","metadata":{"_uuid":"18b04d16-ba61-4e7f-b8b7-01d7e2e415ce","_cell_guid":"a277c174-1d84-40ac-835d-57bb9acf5294","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T14:39:17.249611Z","iopub.execute_input":"2024-12-17T14:39:17.249921Z","iopub.status.idle":"2024-12-17T14:39:31.637144Z","shell.execute_reply.started":"2024-12-17T14:39:17.249894Z","shell.execute_reply":"2024-12-17T14:39:31.636182Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"e83f7ea9-47b9-4a69-b419-71f3c808f848","_cell_guid":"78d1ae54-a469-4b2d-a16f-e5e355332bdd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:20:56.810197Z","iopub.status.idle":"2024-12-17T07:20:56.810694Z","shell.execute_reply.started":"2024-12-17T07:20:56.810424Z","shell.execute_reply":"2024-12-17T07:20:56.810448Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device","metadata":{"_uuid":"6672b61f-85ef-4a4e-b98f-792d1926c70b","_cell_guid":"39d4ccfc-f6b7-48ea-b7dd-74a879f8f5d1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:20:56.811849Z","iopub.status.idle":"2024-12-17T07:20:56.812143Z","shell.execute_reply.started":"2024-12-17T07:20:56.812004Z","shell.execute_reply":"2024-12-17T07:20:56.812019Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models initialization and tokenizations","metadata":{"_uuid":"79a0f677-d886-45c1-b89d-2c07a45bf4e8","_cell_guid":"e47cdfb4-289d-4e12-b5ac-294e51ee05ca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model_name_encoder = \"bert-base-uncased\"","metadata":{"_uuid":"30502618-88d6-445c-a040-0960f3e85c57","_cell_guid":"6876eaed-a7c6-4f47-b81e-e975f8195f9d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T14:39:05.809473Z","iopub.execute_input":"2024-12-17T14:39:05.809799Z","iopub.status.idle":"2024-12-17T14:39:05.813623Z","shell.execute_reply.started":"2024-12-17T14:39:05.809769Z","shell.execute_reply":"2024-12-17T14:39:05.812700Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"context_encoder = AutoModel.from_pretrained(model_name_encoder)\n#this same encoder will be used as the persona encoder but with a linear projection of 16->768","metadata":{"_uuid":"d586623b-daf2-41cb-b771-d42a12d8c086","_cell_guid":"26807645-2810-40a7-ac41-2fd2f4570ca6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:19:53.375926Z","iopub.execute_input":"2024-12-17T03:19:53.376492Z","iopub.status.idle":"2024-12-17T03:19:56.159076Z","shell.execute_reply.started":"2024-12-17T03:19:53.376452Z","shell.execute_reply":"2024-12-17T03:19:56.158171Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context_encoder","metadata":{"_uuid":"ba950c9d-e0d7-4cd2-be21-705f747ebea8","_cell_guid":"51890bf3-d6ba-4584-be32-1b04f4dd2010","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:19:56.160351Z","iopub.execute_input":"2024-12-17T03:19:56.160674Z","iopub.status.idle":"2024-12-17T03:19:56.167507Z","shell.execute_reply.started":"2024-12-17T03:19:56.160646Z","shell.execute_reply":"2024-12-17T03:19:56.166616Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder = GPT2Model.from_pretrained(\"gpt2\")","metadata":{"_uuid":"55669cac-630b-4faf-9f23-647f9c201ced","_cell_guid":"54e5e473-ff2f-48f6-8e51-1f3e545ba070","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:20.675775Z","iopub.execute_input":"2024-12-17T03:20:20.676445Z","iopub.status.idle":"2024-12-17T03:20:23.687234Z","shell.execute_reply.started":"2024-12-17T03:20:20.676412Z","shell.execute_reply":"2024-12-17T03:20:23.686276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder","metadata":{"_uuid":"ca77b275-2858-416a-80a5-fd221c880f96","_cell_guid":"6257ca8a-f10a-4145-83ab-00988be33d6a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:20:23.688655Z","iopub.execute_input":"2024-12-17T03:20:23.688926Z","iopub.status.idle":"2024-12-17T03:20:23.695122Z","shell.execute_reply.started":"2024-12-17T03:20:23.688900Z","shell.execute_reply":"2024-12-17T03:20:23.694216Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder.resize_token_embeddings(decoder.config.vocab_size)","metadata":{"_uuid":"0e73eb23-6ac0-40f2-a603-91ab421ce084","_cell_guid":"ab404c9f-d7c7-46e5-b014-fa03280a9c16","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:33.760098Z","iopub.execute_input":"2024-12-17T03:20:33.760449Z","iopub.status.idle":"2024-12-17T03:20:33.766945Z","shell.execute_reply.started":"2024-12-17T03:20:33.760416Z","shell.execute_reply":"2024-12-17T03:20:33.765990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenizer\ntokenizer_encoder = AutoTokenizer.from_pretrained(model_name_encoder)","metadata":{"_uuid":"f91b8c3f-6e41-43a8-aa84-2fe3770ef6b2","_cell_guid":"393ea443-9a48-428e-8a47-d25c1c57ac94","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:36.795476Z","iopub.execute_input":"2024-12-17T03:20:36.795805Z","iopub.status.idle":"2024-12-17T03:20:37.348150Z","shell.execute_reply.started":"2024-12-17T03:20:36.795774Z","shell.execute_reply":"2024-12-17T03:20:37.347331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer_encoder","metadata":{"_uuid":"89ea9f37-2e4e-40b9-981f-59201349087b","_cell_guid":"1bfb11ae-567b-4087-90cc-d77deb36cb1c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:26:17.376069Z","iopub.execute_input":"2024-12-17T03:26:17.376428Z","iopub.status.idle":"2024-12-17T03:26:17.382803Z","shell.execute_reply.started":"2024-12-17T03:26:17.376396Z","shell.execute_reply":"2024-12-17T03:26:17.381778Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer_decoder = AutoTokenizer.from_pretrained(\"gpt2\")","metadata":{"_uuid":"a2631078-204e-4128-b631-71d0c0b36790","_cell_guid":"f1060f7b-e782-4d4f-89d6-e93d654c37e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:20:40.265321Z","iopub.execute_input":"2024-12-17T03:20:40.265676Z","iopub.status.idle":"2024-12-17T03:20:41.163512Z","shell.execute_reply.started":"2024-12-17T03:20:40.265646Z","shell.execute_reply":"2024-12-17T03:20:41.162824Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set padding token to eos_token for GPT-2\ntokenizer_decoder.pad_token = tokenizer_decoder.eos_token","metadata":{"_uuid":"cad8c52f-4f7a-4a39-987c-0eb7a4a84505","_cell_guid":"fccce296-a407-44b8-851d-413f14a39aab","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:20:43.185513Z","iopub.execute_input":"2024-12-17T03:20:43.185844Z","iopub.status.idle":"2024-12-17T03:20:43.190316Z","shell.execute_reply.started":"2024-12-17T03:20:43.185814Z","shell.execute_reply":"2024-12-17T03:20:43.189179Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learnable projection layer for metacognitive profile\nprofile_projection = nn.Linear(16, context_encoder.config.hidden_size)","metadata":{"_uuid":"c5c87e4f-d68c-4daf-b658-a9b3852a5950","_cell_guid":"ebb66a87-db80-4743-b720-f6d58e5d7c59","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:46.235931Z","iopub.execute_input":"2024-12-17T03:20:46.236867Z","iopub.status.idle":"2024-12-17T03:20:46.244759Z","shell.execute_reply.started":"2024-12-17T03:20:46.236819Z","shell.execute_reply":"2024-12-17T03:20:46.243891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"profile_projection","metadata":{"_uuid":"d1451a9c-d1d4-42fa-92d6-a0401868f09b","_cell_guid":"4f8b1eee-0cef-4dae-b3b8-175f7a90690c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:20:49.345328Z","iopub.execute_input":"2024-12-17T03:20:49.345672Z","iopub.status.idle":"2024-12-17T03:20:49.350971Z","shell.execute_reply.started":"2024-12-17T03:20:49.345643Z","shell.execute_reply":"2024-12-17T03:20:49.350128Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining the PAA layers and Model","metadata":{"_uuid":"403d7dc7-4b5c-4822-86e7-fd478d88a4a6","_cell_guid":"4ba2867a-e595-4253-8f9e-1a3258358822","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class PAALayer(nn.Module):\n    def __init__(self, hidden_size):\n        super(PAALayer, self).__init__()\n        self.cross_attn = nn.MultiheadAttention(hidden_size, num_heads=8)\n        self.sigmoid = nn.Sigmoid()\n        self.linear = nn.Linear(hidden_size * 2, hidden_size)\n\n    def forward(self, persona_hidden, context_hidden, decoder_hidden, tau):\n        c1, _ = self.cross_attn(decoder_hidden, persona_hidden, persona_hidden)\n        c2, _ = self.cross_attn(decoder_hidden, context_hidden, context_hidden)\n        \n        # Adaptive weight calculation\n        w1 = self.sigmoid(self.linear(torch.cat((c1, decoder_hidden), dim=-1)))\n        w2 = 1 - w1\n\n        # Mask creation\n        m1 = torch.where(w1 > tau, 0, 1)\n        m2 = torch.where(w1 < 1 - tau, 0, 1)\n\n        # Weighted summation with masks\n        paa_output = w1 * m1 * c1 + w2 * m2 * c2 + decoder_hidden\n        return paa_output","metadata":{"_uuid":"2590c9e6-7f0c-48cf-8a04-830802fba3fa","_cell_guid":"a320e327-d213-4825-813a-73fc9d40bb41","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:54.715771Z","iopub.execute_input":"2024-12-17T03:20:54.716130Z","iopub.status.idle":"2024-12-17T03:20:54.722624Z","shell.execute_reply.started":"2024-12-17T03:20:54.716099Z","shell.execute_reply":"2024-12-17T03:20:54.721703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PAA_Model(nn.Module):\n    def __init__(self, context_encoder, decoder, profile_projection, paa_layer):\n        super(PAA_Model, self).__init__()\n        self.context_encoder = context_encoder\n        self.decoder = decoder\n        self.profile_projection = profile_projection\n        self.paa_layer = paa_layer\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, context_tokens, target_tokens, profile_vector, tau):\n        # Encode context\n        context_hidden = self.context_encoder(**context_tokens).last_hidden_state\n        print(f\"context_hidden shape: {context_hidden.shape}\")\n\n        # Project metacognitive profile\n        projected_profile = self.profile_projection(profile_vector).unsqueeze(1)\n        print(f\"projected profile shape: {projected_profile.shape}\")\n\n        # Expand persona representation\n        persona_hidden = projected_profile.expand(-1, context_hidden.size(1), -1)\n        print(f\"persona_hidden shape: {persona_hidden.shape}\")\n\n        # Resize or pad persona_hidden and context_hidden to match decoder_hidden length\n        target_length = target_tokens['input_ids'].shape[1]  # Match target sequence length (e.g., decoder_hidden length)\n        context_hidden_resized = self.resize_sequence(context_hidden, target_length)\n        persona_hidden_resized = self.resize_sequence(persona_hidden, target_length)\n\n        print(f\"Resized context_hidden shape: {context_hidden_resized.shape}\")\n        print(f\"Resized persona_hidden shape: {persona_hidden_resized.shape}\")\n\n        # Decoder's output\n        decoder_hidden = self.decoder(**target_tokens).last_hidden_state\n        print(f\"decoder_hidden shape: {decoder_hidden.shape}\")\n\n        # Apply PAA\n        paa_output = self.paa_layer(persona_hidden_resized, context_hidden_resized, decoder_hidden, tau)\n\n        # No softmax here, CrossEntropyLoss expects raw logits\n        logits = paa_output\n        target = target_tokens['input_ids'][:, 1:].contiguous().view(-1)  # Flatten target tokens\n        print(f\"logits shape: {logits.shape}\")\n        print(f\"target shape: {target.shape}\")\n\n        # Ensure logits and target have matching batch size\n        logits = logits.view(-1, logits.size(-1))  # Shape: (batch_size * seq_len, vocab_size)\n\n        assert logits.size(0) == target.size(0), f\"Batch size mismatch: logits batch size {logits.size(0)} vs target batch size {target.size(0)}\"\n\n        # Calculate loss\n        loss = self.loss_fn(logits, target)\n        return loss\n\n    def resize_sequence(self, tensor, target_length):\n        \"\"\"\n        Resize tensor sequence to the target length using padding or interpolation.\n        This method can be adjusted to use either padding or interpolation as per the requirement.\n        \"\"\"\n        current_length = tensor.size(1)\n        if current_length < target_length:\n            # Padding case\n            padding_length = target_length - current_length\n            return F.pad(tensor, (0, 0, 0, padding_length), \"constant\", 0)\n        elif current_length > target_length:\n            # Truncation case\n            return tensor[:, :target_length, :]\n        else:\n            return tensor  # No change if lengths match","metadata":{"_uuid":"a8d7c719-e84b-4a0b-9fc8-2b8a6595ff30","_cell_guid":"4d79acac-fe49-47b5-85b5-cc72e9245e08","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:59.780648Z","iopub.execute_input":"2024-12-17T03:20:59.781367Z","iopub.status.idle":"2024-12-17T03:20:59.790972Z","shell.execute_reply.started":"2024-12-17T03:20:59.781336Z","shell.execute_reply":"2024-12-17T03:20:59.789941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model and optimizer","metadata":{"_uuid":"6e7da9a1-cbfc-4252-b704-54fa3284718a","_cell_guid":"ec759725-103f-4caa-ba2b-053e27a761a1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"paa_layer = PAALayer(context_encoder.config.hidden_size)","metadata":{"_uuid":"9ce494c2-a4b5-4253-824f-29793100e9e5","_cell_guid":"dc990149-545b-4e00-ba99-412c1b1f959a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:21:03.265181Z","iopub.execute_input":"2024-12-17T03:21:03.265531Z","iopub.status.idle":"2024-12-17T03:21:03.295814Z","shell.execute_reply.started":"2024-12-17T03:21:03.265499Z","shell.execute_reply":"2024-12-17T03:21:03.295214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"paa_layer","metadata":{"_uuid":"329df787-7310-4ca3-93f1-cd07d724cde5","_cell_guid":"4de6dcac-7285-430b-aee9-c58f8428b141","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:21:05.705134Z","iopub.execute_input":"2024-12-17T03:21:05.705435Z","iopub.status.idle":"2024-12-17T03:21:05.710899Z","shell.execute_reply.started":"2024-12-17T03:21:05.705408Z","shell.execute_reply":"2024-12-17T03:21:05.710062Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = PAA_Model(context_encoder, decoder, profile_projection, paa_layer).to(device)","metadata":{"_uuid":"aaf3ec0f-092c-4443-b0ff-37b03e54472d","_cell_guid":"b7b44998-9108-4851-90de-97b48752cf17","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:21:26.015247Z","iopub.execute_input":"2024-12-17T03:21:26.015551Z","iopub.status.idle":"2024-12-17T03:21:26.502035Z","shell.execute_reply.started":"2024-12-17T03:21:26.015525Z","shell.execute_reply":"2024-12-17T03:21:26.501334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"_uuid":"d3b6f5ec-e141-49e9-8f63-3eab69163e68","_cell_guid":"e419196b-e304-4c73-a490-d80444e9f7e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:21:28.750644Z","iopub.execute_input":"2024-12-17T03:21:28.750929Z","iopub.status.idle":"2024-12-17T03:21:28.757993Z","shell.execute_reply.started":"2024-12-17T03:21:28.750904Z","shell.execute_reply":"2024-12-17T03:21:28.757152Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = Adam(model.parameters(), lr=5e-5)","metadata":{"_uuid":"6728cace-2cd5-484c-9286-5fd901ae92b2","_cell_guid":"8b7ca88a-639d-4beb-ac82-6d5ab5a784b1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:23:16.496291Z","iopub.execute_input":"2024-12-17T03:23:16.497046Z","iopub.status.idle":"2024-12-17T03:23:16.502307Z","shell.execute_reply.started":"2024-12-17T03:23:16.497014Z","shell.execute_reply":"2024-12-17T03:23:16.501444Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{"_uuid":"cf3edd41-4ce2-46b7-b8ba-665d2ada3230","_cell_guid":"ad28038e-14a2-4375-b356-33bce9e119c8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"file_path = \"/kaggle/input/modified-dataset/modified_dataset.csv\"\ndf = pd.read_csv(file_path)","metadata":{"_uuid":"01326d00-ef5a-402b-b16d-c0ff3e7960c7","_cell_guid":"cc4cef47-43c2-4ddd-bd8e-66597196f7a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T14:38:47.170808Z","iopub.execute_input":"2024-12-17T14:38:47.171217Z","iopub.status.idle":"2024-12-17T14:38:47.227427Z","shell.execute_reply.started":"2024-12-17T14:38:47.171192Z","shell.execute_reply":"2024-12-17T14:38:47.226751Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df.head(5)","metadata":{"_uuid":"35193af2-5f44-4b4e-b78e-15350adc2303","_cell_guid":"4755caab-f924-4d4a-bb57-a2f840b77399","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T14:38:49.639170Z","iopub.execute_input":"2024-12-17T14:38:49.640009Z","iopub.status.idle":"2024-12-17T14:38:49.656668Z","shell.execute_reply.started":"2024-12-17T14:38:49.639976Z","shell.execute_reply":"2024-12-17T14:38:49.655930Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                         description  \\\n0  Create a Python program that performs the foll...   \n1  Create a Python program that accomplishes the ...   \n2  Create a Python program that accomplishes the ...   \n3  Create a Python program that accomplishes the ...   \n4  Create a Python program that accomplishes the ...   \n\n                                        student_code  \\\n0  \"\"\" store the final answer in a variable named...   \n1  \"\"\" store the final answer in a variable named...   \n2  \"\"\" store the final answer in a variable named...   \n3  x=eval(input(\"Enter your age:\"))\\ny=str(input(...   \n4  n = str(input(\"Enter your name:\"))\\na = str(in...   \n\n                                            feedback  \\\n0  [\\n    {\\n    'line_number': 2,\\n    'feedback...   \n1  [\\n    {\\n        'line_number': 4,\\n        '...   \n2  [\\n    {\\n        'line_number': 2,\\n        '...   \n3  [\\n    {\\n        'line_number': 1,\\n        '...   \n4  [\\n    {\\n        'line_number': 3,\\n        '...   \n\n                              metacognitive_feedback  \\\n0  It appears that you are almost on the right tr...   \n1  To improve your solution and better align with...   \n2  Based on your approach to the problem, it seem...   \n3  Based on your approach, it seems like you ofte...   \n4  **Metacognitive Feedback**:\\n\\nYou've made a g...   \n\n                              metacognitive_profile  \n0  [2, 1, 3, 3, 2, 3, 2, 1, 3, 1, 1, 3, 2, 1, 2, 1]  \n1  [3, 1, 2, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 2, 2, 1]  \n2  [2, 1, 1, 2, 2, 3, 3, 1, 3, 3, 2, 1, 2, 1, 2, 2]  \n3  [1, 3, 1, 3, 1, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2]  \n4  [3, 1, 3, 3, 2, 1, 3, 3, 3, 1, 2, 3, 2, 1, 1, 3]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n      <th>student_code</th>\n      <th>feedback</th>\n      <th>metacognitive_feedback</th>\n      <th>metacognitive_profile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Create a Python program that performs the foll...</td>\n      <td>\"\"\" store the final answer in a variable named...</td>\n      <td>[\\n    {\\n    'line_number': 2,\\n    'feedback...</td>\n      <td>It appears that you are almost on the right tr...</td>\n      <td>[2, 1, 3, 3, 2, 3, 2, 1, 3, 1, 1, 3, 2, 1, 2, 1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>\"\"\" store the final answer in a variable named...</td>\n      <td>[\\n    {\\n        'line_number': 4,\\n        '...</td>\n      <td>To improve your solution and better align with...</td>\n      <td>[3, 1, 2, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 2, 2, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>\"\"\" store the final answer in a variable named...</td>\n      <td>[\\n    {\\n        'line_number': 2,\\n        '...</td>\n      <td>Based on your approach to the problem, it seem...</td>\n      <td>[2, 1, 1, 2, 2, 3, 3, 1, 3, 3, 2, 1, 2, 1, 2, 2]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>x=eval(input(\"Enter your age:\"))\\ny=str(input(...</td>\n      <td>[\\n    {\\n        'line_number': 1,\\n        '...</td>\n      <td>Based on your approach, it seems like you ofte...</td>\n      <td>[1, 3, 1, 3, 1, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>n = str(input(\"Enter your name:\"))\\na = str(in...</td>\n      <td>[\\n    {\\n        'line_number': 3,\\n        '...</td>\n      <td>**Metacognitive Feedback**:\\n\\nYou've made a g...</td>\n      <td>[3, 1, 3, 3, 2, 1, 3, 3, 3, 1, 2, 3, 2, 1, 1, 3]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"len(df)","metadata":{"_uuid":"16057344-5f6e-4f61-9c43-266129b302a2","_cell_guid":"1b828457-174d-4f32-83d9-df511b1211a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:25:07.656295Z","iopub.execute_input":"2024-12-17T03:25:07.656641Z","iopub.status.idle":"2024-12-17T03:25:07.662314Z","shell.execute_reply.started":"2024-12-17T03:25:07.656610Z","shell.execute_reply":"2024-12-17T03:25:07.661287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(list(df['description']))","metadata":{"_uuid":"d646c61f-941a-4e1f-a6de-5f345f0af03b","_cell_guid":"152d555b-f34f-4b99-8b07-a45690325097","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:25:44.656848Z","iopub.execute_input":"2024-12-17T03:25:44.657666Z","iopub.status.idle":"2024-12-17T03:25:44.662746Z","shell.execute_reply.started":"2024-12-17T03:25:44.657628Z","shell.execute_reply":"2024-12-17T03:25:44.661923Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\ndef tokenize_data(df, tokenizer_encoder,tokenizer_decoder):\n    context_tokens = tokenizer_encoder(list(df['description']), padding=True, truncation=True, return_tensors=\"pt\")\n    target_tokens = tokenizer_decoder(list(df['metacognitive_feedback']), padding=True, truncation=True, return_tensors=\"pt\")\n    profile_vectors = torch.tensor([ast.literal_eval(profile) for profile in df['metacognitive_profile']], dtype=torch.float)\n    return context_tokens, target_tokens, profile_vectors","metadata":{"_uuid":"fed6a7a7-d6a0-4a9a-a803-530938343446","_cell_guid":"7fdbe31b-907e-4ae7-abdc-d8b0a1a1dcc0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:23:31.896044Z","iopub.execute_input":"2024-12-17T03:23:31.896666Z","iopub.status.idle":"2024-12-17T03:23:31.901631Z","shell.execute_reply.started":"2024-12-17T03:23:31.896632Z","shell.execute_reply":"2024-12-17T03:23:31.900687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context_tokens, target_tokens, profile_vectors = tokenize_data(df, tokenizer_encoder,tokenizer_decoder)","metadata":{"_uuid":"a44c727f-722b-46dd-9874-2bd7b277ee27","_cell_guid":"502a6455-0e9a-4d8b-89eb-bb75a612ce88","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:23:35.760810Z","iopub.execute_input":"2024-12-17T03:23:35.761670Z","iopub.status.idle":"2024-12-17T03:23:36.180240Z","shell.execute_reply.started":"2024-12-17T03:23:35.761631Z","shell.execute_reply":"2024-12-17T03:23:36.179516Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(target_tokens)","metadata":{"_uuid":"0df4ff65-67d0-4367-a393-17b564249cf2","_cell_guid":"99140815-f0ba-4cf8-a945-18d38bbea3b6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:24:22.895921Z","iopub.execute_input":"2024-12-17T03:24:22.896682Z","iopub.status.idle":"2024-12-17T03:24:22.902789Z","shell.execute_reply.started":"2024-12-17T03:24:22.896624Z","shell.execute_reply":"2024-12-17T03:24:22.901713Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"profile_vectors","metadata":{"_uuid":"d5511411-5da8-4e31-b9d0-be4b17b723b7","_cell_guid":"4fc1564d-1e14-4e6a-b2c3-d53d278b40c3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:24:37.355597Z","iopub.execute_input":"2024-12-17T03:24:37.355935Z","iopub.status.idle":"2024-12-17T03:24:37.386198Z","shell.execute_reply.started":"2024-12-17T03:24:37.355905Z","shell.execute_reply":"2024-12-17T03:24:37.385211Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context_tokens = {key: value.to(device) for key, value in context_tokens.items()}\ntarget_tokens = {key: value.to(device) for key, value in target_tokens.items()}\nprofile_vectors = profile_vectors.to(device)","metadata":{"_uuid":"02b04bb3-4dac-4d9d-bedf-93c7e33a65be","_cell_guid":"48d3d450-8fdc-48de-8d99-05587d0d3eac","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:26:46.460774Z","iopub.execute_input":"2024-12-17T03:26:46.461436Z","iopub.status.idle":"2024-12-17T03:26:46.482051Z","shell.execute_reply.started":"2024-12-17T03:26:46.461397Z","shell.execute_reply":"2024-12-17T03:26:46.481185Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataLoader\ntrain_data = TensorDataset(context_tokens['input_ids'], target_tokens['input_ids'], profile_vectors)\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True)","metadata":{"_uuid":"074a07c6-02f9-44e0-8c64-967ca78f8250","_cell_guid":"d655b657-635d-42b9-9c99-8ef9c7c7af02","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:26:54.280836Z","iopub.execute_input":"2024-12-17T03:26:54.281154Z","iopub.status.idle":"2024-12-17T03:26:54.285616Z","shell.execute_reply.started":"2024-12-17T03:26:54.281127Z","shell.execute_reply":"2024-12-17T03:26:54.284698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training loop","metadata":{"_uuid":"94e3aace-7107-4a82-a83f-e6403a3ad4c7","_cell_guid":"a1b089b5-2fdf-4b22-ac39-fe4f7d0d368a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model.train()\nnum_epochs = 5\ntau = 0.5\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for context_ids, target_ids, profile_vector in train_loader:\n        # Move tensors to the GPU (if available)\n        context_ids = context_ids.to(device)\n        target_ids = target_ids.to(device)\n        profile_vector = profile_vector.to(device)\n\n        optimizer.zero_grad()\n\n        # Prepare input tensors\n        context_tokens = {'input_ids': context_ids, 'attention_mask': context_ids != tokenizer_encoder.pad_token_id}\n        target_tokens = {'input_ids': target_ids, 'attention_mask': target_ids != tokenizer_decoder.pad_token_id}\n\n        # Forward pass through the model\n        loss = model(context_tokens, target_tokens, profile_vector, tau)\n\n        # Backpropagate and update the model\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")","metadata":{"_uuid":"bf476f50-8f76-4429-afd0-2f47630fa9dc","_cell_guid":"2347f634-be12-45b1-8d79-d1f0bb114c8a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:26:58.135783Z","iopub.execute_input":"2024-12-17T03:26:58.136128Z","iopub.status.idle":"2024-12-17T03:27:00.030683Z","shell.execute_reply.started":"2024-12-17T03:26:58.136098Z","shell.execute_reply":"2024-12-17T03:27:00.029527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"_uuid":"7a922bc4-5777-4a1c-9e5a-bd820b02e268","_cell_guid":"e6548ec5-b73a-48ec-832e-c1865f3b9fb3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:07:38.511418Z","iopub.execute_input":"2024-12-16T17:07:38.512324Z","iopub.status.idle":"2024-12-16T17:07:38.517039Z","shell.execute_reply.started":"2024-12-16T17:07:38.512265Z","shell.execute_reply":"2024-12-16T17:07:38.516219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.memory_summary())","metadata":{"_uuid":"ec7b2129-3b55-4f70-985c-b764d7c43a0e","_cell_guid":"c57c0ba7-bba8-4ebc-9c2e-ef5ad6ca52c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:11:28.010818Z","iopub.execute_input":"2024-12-16T17:11:28.011421Z","iopub.status.idle":"2024-12-16T17:11:28.016583Z","shell.execute_reply.started":"2024-12-16T17:11:28.011387Z","shell.execute_reply":"2024-12-16T17:11:28.015637Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Context Encoder","metadata":{"_uuid":"a54f4e1e-7bac-4e3e-9456-28a9f70ef0c0","_cell_guid":"acd7f754-97d2-472f-8cd8-8a17691e400d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"encoder = BertModel.from_pretrained(model_name_encoder)","metadata":{"_uuid":"a39c6371-5059-45b5-9ab1-f028d7456565","_cell_guid":"06d0cb18-5076-4128-aa7d-4ebeb504be81","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T14:39:41.491588Z","iopub.execute_input":"2024-12-17T14:39:41.493047Z","iopub.status.idle":"2024-12-17T14:39:43.975746Z","shell.execute_reply.started":"2024-12-17T14:39:41.492990Z","shell.execute_reply":"2024-12-17T14:39:43.974889Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a27a53859af4f17bd78f4daff757d62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1018aea2edff4f399de43239c15787a2"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenizer_encoder = BertTokenizer.from_pretrained(model_name_encoder)","metadata":{"_uuid":"a89927c0-4e9a-47aa-a9dd-fcc6bb02f3b1","_cell_guid":"10fd9213-45db-4d6a-a89c-6231690ce158","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T14:39:47.595183Z","iopub.execute_input":"2024-12-17T14:39:47.595523Z","iopub.status.idle":"2024-12-17T14:39:48.630395Z","shell.execute_reply.started":"2024-12-17T14:39:47.595492Z","shell.execute_reply":"2024-12-17T14:39:48.629769Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30fd5363447e4fac91226787468caf74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec26976e9024d87b4b66a779b6eafe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63e5c69dd92e432496109517c5b86d2e"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Function to encode context\ndef encode_context(description, student_code, feedback, max_len=512):\n    context = f\"Description: {description} Student Code: {student_code} Feedback: {feedback}\"\n    inputs = tokenizer_encoder(\n        context, \n        padding='max_length', \n        truncation=True, \n        max_length=max_len, \n        return_tensors='pt'\n    )\n    return inputs","metadata":{"_uuid":"41b25e5d-60fe-4bb1-b89c-bbeed9ff13ee","_cell_guid":"92a5215c-abaa-445b-a0ac-278dfd56b1a1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T14:39:54.592641Z","iopub.execute_input":"2024-12-17T14:39:54.593474Z","iopub.status.idle":"2024-12-17T14:39:54.598039Z","shell.execute_reply.started":"2024-12-17T14:39:54.593442Z","shell.execute_reply":"2024-12-17T14:39:54.597122Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"encoded_context = []","metadata":{"_uuid":"ce0f4149-0c29-4f74-ba75-aa50b1564395","_cell_guid":"e53555c7-178b-47af-87b8-53707eb22005","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:22:11.179797Z","iopub.execute_input":"2024-12-17T07:22:11.180927Z","iopub.status.idle":"2024-12-17T07:22:11.184979Z","shell.execute_reply.started":"2024-12-17T07:22:11.180881Z","shell.execute_reply":"2024-12-17T07:22:11.184152Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for idx, row in df.iterrows():\n    description = row['description']\n    student_code = row['student_code']\n    feedback = row['feedback']\n    \n    # Encode context (IU)\n    context_inputs = encode_context(student_code, feedback, description)\n\n    # Get BERT hidden states\n    with torch.no_grad():\n        hU = encoder(**context_inputs).last_hidden_state  # Shape: (1, max_len, 768)\n\n    encoded_context.append(hU.squeeze(0))\n    print(len(encoded_context))","metadata":{"_uuid":"baa2843a-52f7-43fd-8aac-d54f8a28d8c5","_cell_guid":"05dee144-6050-48e0-ae36-94902555b9ca","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:23:49.835039Z","iopub.execute_input":"2024-12-17T07:23:49.835345Z","iopub.status.idle":"2024-12-17T07:26:42.829537Z","shell.execute_reply.started":"2024-12-17T07:23:49.835317Z","shell.execute_reply":"2024-12-17T07:26:42.828658Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Encoded Context Shape: {encoded_context[0].shape}\")","metadata":{"_uuid":"adb19207-b959-45dc-8128-13c6bd21c436","_cell_guid":"ec4ca9f1-48a4-4d45-9403-eeadd1dbcd47","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:29:02.920115Z","iopub.execute_input":"2024-12-17T07:29:02.920960Z","iopub.status.idle":"2024-12-17T07:29:02.925332Z","shell.execute_reply.started":"2024-12-17T07:29:02.920926Z","shell.execute_reply":"2024-12-17T07:29:02.924286Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_context[0]","metadata":{"_uuid":"d7098c76-9110-4f46-8f58-0aa1c5ee0fe9","_cell_guid":"9a0f8b1a-f1c1-40e5-a6b9-969b0763a0c9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:29:29.144538Z","iopub.execute_input":"2024-12-17T07:29:29.145175Z","iopub.status.idle":"2024-12-17T07:29:29.212816Z","shell.execute_reply.started":"2024-12-17T07:29:29.145144Z","shell.execute_reply":"2024-12-17T07:29:29.212099Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ContextEncoder(nn.Module):\n    def __init__(self, bert_model_name='bert-base-uncased', output_dim=768):\n        super(ContextEncoder, self).__init__()\n        \n        # BERT model for encoding the context (student_code, feedback, description)\n        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n        self.bert_encoder = BertModel.from_pretrained(bert_model_name)\n        \n        # Optional: Linear layer to project BERT output to the desired dimension\n        self.fc = nn.Linear(self.bert_encoder.config.hidden_size, output_dim)\n\n    def forward(self, description, student_code, feedback):\n        # Combine the context information (description, student_code, feedback)\n        context = f\"Description: {description} Student Code: {student_code} Feedback: {feedback}\"\n        \n        # Tokenize the input context and prepare it for BERT\n        encoded_inputs = self.tokenizer(\n            context, \n            return_tensors='pt', \n            padding='max_length', \n            truncation=True, \n            max_length=512\n        )\n        \n        # Encode the context using BERT\n        with torch.no_grad():\n            context_hidden_states = self.bert_encoder(**encoded_inputs).last_hidden_state  # (batch_size, seq_len, hidden_dim)\n        \n        # Take the mean of the hidden states across the sequence length to get a fixed-size representation\n        context_rep = context_hidden_states.mean(dim=1)  # (batch_size, hidden_dim)\n        \n        context_rep = self.fc(context_rep)  # (batch_size, output_dim)\n        \n        return context_rep","metadata":{"_uuid":"f74b60da-0791-433b-8273-8bbc2e262947","_cell_guid":"a0554a5d-0cc3-42b0-afe9-b3605296e57b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T14:40:09.870627Z","iopub.execute_input":"2024-12-17T14:40:09.870998Z","iopub.status.idle":"2024-12-17T14:40:09.877474Z","shell.execute_reply.started":"2024-12-17T14:40:09.870965Z","shell.execute_reply":"2024-12-17T14:40:09.876580Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"context_encoder = ContextEncoder()","metadata":{"_uuid":"4c808304-de9e-434f-8b57-1d795efcf084","_cell_guid":"7ac1b345-eb9a-43db-86b2-90aa33d65e6e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T14:40:12.409175Z","iopub.execute_input":"2024-12-17T14:40:12.409518Z","iopub.status.idle":"2024-12-17T14:40:12.847396Z","shell.execute_reply.started":"2024-12-17T14:40:12.409480Z","shell.execute_reply":"2024-12-17T14:40:12.846750Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"context=[]","metadata":{"_uuid":"dafbf2d2-65d0-4960-8db4-8549dce8dabb","_cell_guid":"95056cd7-603c-4170-84c5-9e517139add4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:49:07.030213Z","iopub.execute_input":"2024-12-17T07:49:07.030889Z","iopub.status.idle":"2024-12-17T07:49:07.034392Z","shell.execute_reply.started":"2024-12-17T07:49:07.030855Z","shell.execute_reply":"2024-12-17T07:49:07.033535Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for idx, row in df.iterrows():\n    description = row['description']\n    student_code = row['student_code']\n    feedback = row['feedback']\n    \n    # Get the context representation by passing the description, student_code, and feedback\n    context_rep = context_encoder(description, student_code, feedback)\n    \n    # Append the result to the encoded context list\n    context.append(context_rep)\n    print(len(context))","metadata":{"_uuid":"e56e7167-679e-4349-9964-71fe45a20438","_cell_guid":"3f9e3a8a-5ba8-4e28-98f4-7478876313b9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:49:52.064721Z","iopub.execute_input":"2024-12-17T07:49:52.065056Z","iopub.status.idle":"2024-12-17T07:52:50.698070Z","shell.execute_reply.started":"2024-12-17T07:49:52.065024Z","shell.execute_reply":"2024-12-17T07:52:50.697133Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(context[0].shape)","metadata":{"_uuid":"bf333806-c81f-45ee-b5ba-7d19dafdeb5e","_cell_guid":"f17a0c4a-03e8-4a08-afab-421d2aa48612","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:53:45.670548Z","iopub.execute_input":"2024-12-17T07:53:45.671362Z","iopub.status.idle":"2024-12-17T07:53:45.676745Z","shell.execute_reply.started":"2024-12-17T07:53:45.671308Z","shell.execute_reply":"2024-12-17T07:53:45.675709Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context[0]","metadata":{"_uuid":"f38b8ebf-bf7d-4f7b-a626-64088718109c","_cell_guid":"9cbab324-f885-4773-b26b-db511b8df145","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:04:07.314666Z","iopub.execute_input":"2024-12-17T08:04:07.315591Z","iopub.status.idle":"2024-12-17T08:04:07.326179Z","shell.execute_reply.started":"2024-12-17T08:04:07.315554Z","shell.execute_reply":"2024-12-17T08:04:07.325321Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Persona Encoder","metadata":{"_uuid":"c361dd0d-650e-476f-ba5e-7e2d0b828f5a","_cell_guid":"dc447f6d-ee00-42b3-b5cf-ad411c982f33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class PersonaEncoder(nn.Module):\n    def __init__(self, metacognitive_dim=16, output_dim=768):\n        super(PersonaEncoder, self).__init__()\n        \n        # Linear layer to map the metacognitive vector to the output dimension\n        self.metacognitive_fc = nn.Linear(metacognitive_dim, output_dim)  # from 16 to 768\n        \n        # Final projection layer to further process the output\n        self.final_fc = nn.Linear(output_dim, output_dim)\n\n    def forward(self, metacognitive_vector):\n        # Process the metacognitive vector through the linear layer\n        metacognitive_rep = self.metacognitive_fc(metacognitive_vector)  # (batch_size, output_dim)\n        \n        # Optionally, apply another linear layer or activation if necessary\n        final_rep = self.final_fc(metacognitive_rep)  # (batch_size, output_dim)\n        \n        return final_rep","metadata":{"_uuid":"223b0a76-4222-40ab-bc06-b0fb4508f33e","_cell_guid":"be7a4518-16cf-4c13-bc91-36dbdca41905","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T14:40:16.842855Z","iopub.execute_input":"2024-12-17T14:40:16.843190Z","iopub.status.idle":"2024-12-17T14:40:16.848439Z","shell.execute_reply.started":"2024-12-17T14:40:16.843166Z","shell.execute_reply":"2024-12-17T14:40:16.847468Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"persona_encoder = PersonaEncoder()","metadata":{"_uuid":"2b06f448-b5eb-478b-ad21-7d131941f112","_cell_guid":"3a73de3f-bb58-4b0f-b60a-139c010d8769","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T14:40:19.269282Z","iopub.execute_input":"2024-12-17T14:40:19.269625Z","iopub.status.idle":"2024-12-17T14:40:19.280103Z","shell.execute_reply.started":"2024-12-17T14:40:19.269594Z","shell.execute_reply":"2024-12-17T14:40:19.279134Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"encoded_persona=[]","metadata":{"_uuid":"b6141891-7c7b-4341-b2b0-96aa8cccde8f","_cell_guid":"a94c268f-1ae0-4f1e-bbc2-8b6293ba70e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:56:24.749784Z","iopub.execute_input":"2024-12-17T07:56:24.750419Z","iopub.status.idle":"2024-12-17T07:56:24.753758Z","shell.execute_reply.started":"2024-12-17T07:56:24.750386Z","shell.execute_reply":"2024-12-17T07:56:24.752959Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\nfor idx, row in df.iterrows():\n        # Convert the string representation of the list to an actual list of integers\n        #metacognitive_vector = ast.literal_eval(row['metacognitive_profile'])\n        metacognitive_tensor = torch.tensor([ast.literal_eval(profile) for profile in df['metacognitive_profile']], dtype=torch.float)\n        \n        # Convert to a PyTorch tensor of shape (1, 16)\n        #metacognitive_tensor = torch.tensor(metacognitive_vector, dtype=torch.float32).unsqueeze(0)\n        \n        # Encode using the PersonaEncoder\n        persona_rep = persona_encoder(metacognitive_tensor)\n        \n        # Append result to the list\n        encoded_persona.append(persona_rep)\n        print(f\"Encoded persona {idx+1}/{len(df)}\")","metadata":{"_uuid":"95cb66ff-8644-4921-b0ef-bbcf389bf474","_cell_guid":"e1942726-3cc6-4c84-9de7-c48e85378431","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:02:57.989349Z","iopub.execute_input":"2024-12-17T08:02:57.989739Z","iopub.status.idle":"2024-12-17T08:03:03.138744Z","shell.execute_reply.started":"2024-12-17T08:02:57.989710Z","shell.execute_reply":"2024-12-17T08:03:03.137709Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_persona[0].shape","metadata":{"_uuid":"6557deba-b0b1-4234-95fe-5d72fcb9ecb7","_cell_guid":"ebc0d3ff-25d6-4bd6-8ccb-e025ed999891","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:03:34.274281Z","iopub.execute_input":"2024-12-17T08:03:34.274968Z","iopub.status.idle":"2024-12-17T08:03:34.280398Z","shell.execute_reply.started":"2024-12-17T08:03:34.274934Z","shell.execute_reply":"2024-12-17T08:03:34.279572Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_persona[365]","metadata":{"_uuid":"f2436afd-1a2e-4d55-ad22-120412ab404b","_cell_guid":"294930e9-61c7-4e4b-aca3-1670267381fa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:04:50.395796Z","iopub.execute_input":"2024-12-17T08:04:50.396482Z","iopub.status.idle":"2024-12-17T08:04:50.403762Z","shell.execute_reply.started":"2024-12-17T08:04:50.396447Z","shell.execute_reply":"2024-12-17T08:04:50.402909Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(encoded_persona)","metadata":{"_uuid":"54304fe4-1ac4-4b49-a40e-74f31032d9bb","_cell_guid":"52ebe085-8006-41e2-ad60-f2a09a2df698","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:04:38.230904Z","iopub.execute_input":"2024-12-17T08:04:38.231243Z","iopub.status.idle":"2024-12-17T08:04:38.236596Z","shell.execute_reply.started":"2024-12-17T08:04:38.231210Z","shell.execute_reply":"2024-12-17T08:04:38.235614Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Stack context and persona encodings into tensors\nencoded_context_tensor = torch.cat(encoded_context, dim=0)  # Shape: [N, 768]\nencoded_persona_tensor = torch.stack(encoded_persona, dim=0)  # Shape: [M, 768]","metadata":{"_uuid":"246e78f1-1385-4913-9bef-cdba468b436a","_cell_guid":"02207754-99c9-42c0-95c3-a60afcbe9d9f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:09:25.865167Z","iopub.execute_input":"2024-12-17T08:09:25.865852Z","iopub.status.idle":"2024-12-17T08:09:26.350915Z","shell.execute_reply.started":"2024-12-17T08:09:25.865817Z","shell.execute_reply":"2024-12-17T08:09:26.349904Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"the encoded context tensor dimensions:\",encoded_context_tensor.shape)","metadata":{"_uuid":"a629c5c9-ceed-466e-bdea-9c9afe4a2388","_cell_guid":"cfbf5ca5-b31b-4f46-8e61-15876d0b4e01","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:12:53.626625Z","iopub.execute_input":"2024-12-17T08:12:53.627077Z","iopub.status.idle":"2024-12-17T08:12:53.632972Z","shell.execute_reply.started":"2024-12-17T08:12:53.627032Z","shell.execute_reply":"2024-12-17T08:12:53.631930Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"the encoded persona tensor dimensions:\",encoded_persona_tensor.shape)","metadata":{"_uuid":"80d6c22d-7b2e-4158-b4db-31f4ad1d707f","_cell_guid":"72570788-ab31-4285-aafa-77079b364eae","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:12:55.935278Z","iopub.execute_input":"2024-12-17T08:12:55.935887Z","iopub.status.idle":"2024-12-17T08:12:55.940470Z","shell.execute_reply.started":"2024-12-17T08:12:55.935855Z","shell.execute_reply":"2024-12-17T08:12:55.939656Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"71d4f913-ea96-4832-a7f9-100ca1640c91","_cell_guid":"7331676f-d099-43ce-aed8-1eab6262ecd2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PAA Layer","metadata":{"_uuid":"04a41811-8b4d-43e4-9be4-7594ab95dc60","_cell_guid":"f3198c17-0559-4904-9269-17c05a196160","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"* hR : Slef attention obtained by the metacognitive reposnse + metacognitive vector\n* oP : cross attention result from (hR, hP)\n* oC : cross attention result from (hR, hC)\n* persona_length : metacognitive vector encoded tensor length\n* context_length : context(description + student code + general feedback) encoded tensor length","metadata":{}},{"cell_type":"code","source":"class PAALayer(nn.Module):\n    def __init__(self, hidden_dimension = 768 , tau=0.5):\n        super(PAALayer, self).__init__()\n        self.hidden_dimenstion = hidden_dimension\n        self.tau = tau\n        \n        # Linear layers for generating the initial weights (wpersona)\n        self.fc = nn.Linear(2 * hidden_dimension, hidden_dimension)  # Concatenated hR and oP\n        self.sigmoid = nn.Sigmoid()\n\n\n    def forward(self, hR , oP, oC):\n        #cross attention results\n        Mp_input  = torch.cat([hR,oP], dim=0)#1\n        Mp = self.fc(Mp_input)#2\n        Wp = self.sigmoid(Mp) #3\n\n        #apply weighting = 4\n        Mpersona = (Wp > self.tau).float()\n        Mcontext = (1 - Wp > self.tau).float()\n\n        #apply masking to the cross attention / element wise multiplication = 5\n        oP_weighted = Mpersona.unsqueeze(-1) * oP\n        oC_weighted = Mcontext.unsqueeze(-1) * oC\n\n        #adding the cross attention results = 6\n        HPAA = oP_weighted + oC_weighted #output resulted from PAA layers\n        return HPAA     \n        \n\n        \n\n        ","metadata":{"_uuid":"38e22573-3027-4a51-98b8-e533d286fc0d","_cell_guid":"98759362-3917-4709-afe7-cebc9c9a4513","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T14:42:26.725958Z","iopub.execute_input":"2024-12-17T14:42:26.726656Z","iopub.status.idle":"2024-12-17T14:42:26.732804Z","shell.execute_reply.started":"2024-12-17T14:42:26.726620Z","shell.execute_reply":"2024-12-17T14:42:26.731890Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# PAA Model","metadata":{}},{"cell_type":"markdown","source":"* metacognitive_response: Tensor of shape (batch_size, seq_len, hidden_size)\n* metacognitive_vector: Tensor of shape (batch_size, hidden_size)\n* encoded_context: Tensor of shape (batch_size, context_len, hidden_size)\n* encoded_persona: Tensor of shape (batch_size, persona_len, hidden_size)","metadata":{}},{"cell_type":"code","source":"class PAAModel(nn.Module):\n    def __init__(self, hidden_size=768, vocab_size = 30522 ,tau=0.5, max_length=512):\n        super(PAAModel , self).__init__()\n        self.hidden_size = hidden_size\n        self.tau = tau\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n\n        #transformer decoder to decode the metacognitive response\n        self.transformer_decoder = nn.TransformerDecoderLayer(d_model = hidden_size , nhead=8)\n        self.decoder = nn.TransformerDecoder(self.transformer_decoder, num_layers=6)\n        #self.decoder = GPT2LMHeadModel.from_pretrained('gpt2')\n\n\n        # Attention layers for persona and context\n        self.context_attn = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.persona_attn = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        \n        #linear FC output layer\n        self.fc_out = nn.Linear(hidden_size, vocab_size)\n        #PAA layer\n        self.paa = PAALayer(hidden_dimension=hidden_size, tau=tau)\n\n    def forward(self, metacognitive_response , metacognitive_vector, encoded_context , encoded_persona):\n        #deocder input - concatenate reposnse + vector = step 1\n        decoder_input = torch.cat([metacognitive_response, metacognitive_vector.unsqueeze(1)], dim=1)\n        print(\"decoder input shape:\" ,decoder_input.shape)\n\n        #step 2 - self attention on input embedding\n        hR = self.decoder(decoder_input, memory = None)\n        print(\"hR shape:\" , hR.shape)\n\n        #step 3 = cross attention\n        oP, _ = self.persona_attn(hR, encoded_persona, encoded_persona)\n        oC, _ = self.context_attn(hR, encoded_context, encoded_context)\n        print(\"oP shape:\" , oP.shape)\n        print(\"oC shape:\" , oC.shape)\n\n\n        #step 4 = apply PAA layer\n        HPAA = self.paa(hR , oU, oC)\n        print(\"HPAA shape:\"  , HPAA.shape)\n\n        #step 5 = linear output\n        logits = self.fc_out(HPAA)\n        print(\"logits shape:\" , logits.shape)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:41:06.527835Z","iopub.execute_input":"2024-12-17T14:41:06.528170Z","iopub.status.idle":"2024-12-17T14:41:06.536698Z","shell.execute_reply.started":"2024-12-17T14:41:06.528143Z","shell.execute_reply":"2024-12-17T14:41:06.535768Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"model = PAAModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:42:31.029411Z","iopub.execute_input":"2024-12-17T14:42:31.030232Z","iopub.status.idle":"2024-12-17T14:42:31.430940Z","shell.execute_reply.started":"2024-12-17T14:42:31.030201Z","shell.execute_reply":"2024-12-17T14:42:31.429931Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:42:34.425162Z","iopub.execute_input":"2024-12-17T14:42:34.425982Z","iopub.status.idle":"2024-12-17T14:42:34.430626Z","shell.execute_reply.started":"2024-12-17T14:42:34.425946Z","shell.execute_reply":"2024-12-17T14:42:34.429717Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"LOSS = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:42:37.044837Z","iopub.execute_input":"2024-12-17T14:42:37.045505Z","iopub.status.idle":"2024-12-17T14:42:37.049144Z","shell.execute_reply.started":"2024-12-17T14:42:37.045474Z","shell.execute_reply":"2024-12-17T14:42:37.048302Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"num_epochs = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:43:23.665154Z","iopub.execute_input":"2024-12-17T14:43:23.665478Z","iopub.status.idle":"2024-12-17T14:43:23.669521Z","shell.execute_reply.started":"2024-12-17T14:43:23.665452Z","shell.execute_reply":"2024-12-17T14:43:23.668553Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import ast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:43:43.210453Z","iopub.execute_input":"2024-12-17T14:43:43.210849Z","iopub.status.idle":"2024-12-17T14:43:43.214827Z","shell.execute_reply.started":"2024-12-17T14:43:43.210819Z","shell.execute_reply":"2024-12-17T14:43:43.213984Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for idx,row in df.iterrows():\n        description = row['description']\n        student_code = row['student_code']\n        feedback = row['feedback']\n        metacognitive_feedback = row['metacognitive_feedback']\n        metacognitive_vector= row['metacognitive_profile']\n\n\n        metacognitive_tensor = torch.tensor([\n            ast.literal_eval(profile) \n            for profile in df['metacognitive_profile']],\n                                            dtype=torch.float)\n\n        context_encoding = context_encoder(description, student_code , feedback)\n        persona_encoding = persona_encoder(metacognitive_tensor)\n\n        logits = model(metacognitive_feedback, metacognitive_tensor, context_encoding, persona_encoding)\n\n        loss = LOSS(logits.view(-1, model.vocab_size), target_labels.view(-1))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(df):.4f}\")\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:44:13.106621Z","iopub.execute_input":"2024-12-17T14:44:13.107266Z","iopub.status.idle":"2024-12-17T14:44:13.844446Z","shell.execute_reply.started":"2024-12-17T14:44:13.107224Z","shell.execute_reply":"2024-12-17T14:44:13.843250Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m context_encoding \u001b[38;5;241m=\u001b[39m context_encoder(description, student_code , feedback)\n\u001b[1;32m     19\u001b[0m persona_encoding \u001b[38;5;241m=\u001b[39m persona_encoder(metacognitive_tensor)\n\u001b[0;32m---> 21\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetacognitive_feedback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetacognitive_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersona_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m LOSS(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mvocab_size), target_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mPAAModel.forward\u001b[0;34m(self, metacognitive_response, metacognitive_vector, encoded_context, encoded_persona)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, metacognitive_response , metacognitive_vector, encoded_context , encoded_persona):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#deocder input - concatenate reposnse + vector = step 1\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetacognitive_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetacognitive_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder input shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m ,decoder_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#step 2 - self attention on input embedding\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got str"],"ename":"TypeError","evalue":"expected Tensor as element 0 in argument 0, but got str","output_type":"error"}],"execution_count":35},{"cell_type":"markdown","source":"# Inferencing","metadata":{}},{"cell_type":"code","source":"def inference(model, context_encoder, persona_encoder, description, student_code, feedback, metacognitive_feedback):\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():  # Disable gradient computation for inference\n        # Step 1: Convert the input data to the necessary tensors\n        metacognitive_tensor = torch.tensor(\n            [ast.literal_eval(metacognitive_feedback)], dtype=torch.float\n        )\n\n        # Step 2: Encode context and persona using the respective encoders\n        context_encoding = context_encoder(description, student_code, feedback)  # Shape: [1, 768]\n        persona_encoding = persona_encoder(metacognitive_tensor)  # Shape: [1, 768]\n\n        # Step 3: Prepare the metacognitive response (this is the input for the decoder)\n        metacognitive_response = torch.tensor(\n            [ast.literal_eval(metacognitive_feedback)], dtype=torch.float\n        )  # Assuming metacognitive_feedback is in the required format\n\n        # Step 4: Pass everything through the model (no gradients needed for inference)\n        logits = model(metacognitive_response, metacognitive_tensor, context_encoding, persona_encoding)\n\n        # Step 5: Convert logits to predictions (for simplicity, let's take the argmax here)\n        predictions = torch.argmax(logits, dim=-1)  # Taking argmax along the vocabulary dimension\n\n        # Step 6: If needed, convert the predicted tokens back to text (this step depends on your output format)\n        predicted_tokens = predictions.squeeze().tolist()\n\n        # Convert tokens to text (if using a tokenizer for language generation, replace this step)\n        # For simplicity, assuming the output is a sequence of tokens\n        return predicted_tokens\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row = df.iloc[0]  \n\ndescription = row['description']\nstudent_code = row['student_code']\nfeedback = row['feedback']\nmetacognitive_feedback = row['metacognitive_feedback']\nmetacognitive_vector= row['metacognitive_profile']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_tokens = inference(model, context_encoder, persona_encoder, description, student_code, feedback, metacognitive_feedback)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Predicted tokens: {predicted_tokens}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}