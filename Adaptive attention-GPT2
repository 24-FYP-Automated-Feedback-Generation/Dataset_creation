{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10048507,"sourceType":"datasetVersion","datasetId":6190931}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"a253fd50-0c38-4ed2-95be-d8de722c76c0","_cell_guid":"879296a9-98ee-4c26-be52-e01b545a2867","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-18T12:00:44.963922Z","iopub.execute_input":"2024-12-18T12:00:44.964634Z","iopub.status.idle":"2024-12-18T12:00:45.276532Z","shell.execute_reply.started":"2024-12-18T12:00:44.964598Z","shell.execute_reply":"2024-12-18T12:00:45.275630Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/modified-dataset/modified_dataset.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Import libraries.","metadata":{"_uuid":"1ad07d1c-cedd-4a5a-b877-8725368c6600","_cell_guid":"74f900c3-cd95-40c7-bae9-4b6e72d883a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"pip install transformers torch","metadata":{"_uuid":"faf8192f-ccd7-4e28-ab6c-eac95a4dbbd4","_cell_guid":"19421aeb-0fda-47bb-8161-ce259118872e","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:00:47.560335Z","iopub.execute_input":"2024-12-18T12:00:47.561123Z","iopub.status.idle":"2024-12-18T12:00:57.439140Z","shell.execute_reply.started":"2024-12-18T12:00:47.561075Z","shell.execute_reply":"2024-12-18T12:00:57.438022Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoModel, AutoTokenizer, GPT2Model,GPT2Tokenizer,GPT2LMHeadModel\nfrom transformers import BertModel, BertTokenizer","metadata":{"_uuid":"18b04d16-ba61-4e7f-b8b7-01d7e2e415ce","_cell_guid":"a277c174-1d84-40ac-835d-57bb9acf5294","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-18T12:00:57.440984Z","iopub.execute_input":"2024-12-18T12:00:57.441278Z","iopub.status.idle":"2024-12-18T12:01:12.522914Z","shell.execute_reply.started":"2024-12-18T12:00:57.441248Z","shell.execute_reply":"2024-12-18T12:01:12.522217Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"e83f7ea9-47b9-4a69-b419-71f3c808f848","_cell_guid":"78d1ae54-a469-4b2d-a16f-e5e355332bdd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T12:01:12.523809Z","iopub.execute_input":"2024-12-18T12:01:12.524236Z","iopub.status.idle":"2024-12-18T12:01:12.610366Z","shell.execute_reply.started":"2024-12-18T12:01:12.524211Z","shell.execute_reply":"2024-12-18T12:01:12.609452Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device","metadata":{"_uuid":"6672b61f-85ef-4a4e-b98f-792d1926c70b","_cell_guid":"39d4ccfc-f6b7-48ea-b7dd-74a879f8f5d1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T12:01:12.612587Z","iopub.execute_input":"2024-12-18T12:01:12.613018Z","iopub.status.idle":"2024-12-18T12:01:12.637098Z","shell.execute_reply.started":"2024-12-18T12:01:12.612969Z","shell.execute_reply":"2024-12-18T12:01:12.636207Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Models initialization and tokenizations","metadata":{"_uuid":"79a0f677-d886-45c1-b89d-2c07a45bf4e8","_cell_guid":"e47cdfb4-289d-4e12-b5ac-294e51ee05ca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model_name_encoder = \"bert-base-uncased\"","metadata":{"_uuid":"30502618-88d6-445c-a040-0960f3e85c57","_cell_guid":"6876eaed-a7c6-4f47-b81e-e975f8195f9d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-18T12:01:16.647789Z","iopub.execute_input":"2024-12-18T12:01:16.648581Z","iopub.status.idle":"2024-12-18T12:01:16.652282Z","shell.execute_reply.started":"2024-12-18T12:01:16.648548Z","shell.execute_reply":"2024-12-18T12:01:16.651252Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"context_encoder = AutoModel.from_pretrained(model_name_encoder)\n#this same encoder will be used as the persona encoder but with a linear projection of 16->768","metadata":{"_uuid":"d586623b-daf2-41cb-b771-d42a12d8c086","_cell_guid":"26807645-2810-40a7-ac41-2fd2f4570ca6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:19:53.375926Z","iopub.execute_input":"2024-12-17T03:19:53.376492Z","iopub.status.idle":"2024-12-17T03:19:56.159076Z","shell.execute_reply.started":"2024-12-17T03:19:53.376452Z","shell.execute_reply":"2024-12-17T03:19:56.158171Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context_encoder","metadata":{"_uuid":"ba950c9d-e0d7-4cd2-be21-705f747ebea8","_cell_guid":"51890bf3-d6ba-4584-be32-1b04f4dd2010","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:19:56.160351Z","iopub.execute_input":"2024-12-17T03:19:56.160674Z","iopub.status.idle":"2024-12-17T03:19:56.167507Z","shell.execute_reply.started":"2024-12-17T03:19:56.160646Z","shell.execute_reply":"2024-12-17T03:19:56.166616Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder = GPT2Model.from_pretrained(\"gpt2\")","metadata":{"_uuid":"55669cac-630b-4faf-9f23-647f9c201ced","_cell_guid":"54e5e473-ff2f-48f6-8e51-1f3e545ba070","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:20.675775Z","iopub.execute_input":"2024-12-17T03:20:20.676445Z","iopub.status.idle":"2024-12-17T03:20:23.687234Z","shell.execute_reply.started":"2024-12-17T03:20:20.676412Z","shell.execute_reply":"2024-12-17T03:20:23.686276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder","metadata":{"_uuid":"ca77b275-2858-416a-80a5-fd221c880f96","_cell_guid":"6257ca8a-f10a-4145-83ab-00988be33d6a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:20:23.688655Z","iopub.execute_input":"2024-12-17T03:20:23.688926Z","iopub.status.idle":"2024-12-17T03:20:23.695122Z","shell.execute_reply.started":"2024-12-17T03:20:23.688900Z","shell.execute_reply":"2024-12-17T03:20:23.694216Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder.resize_token_embeddings(decoder.config.vocab_size)","metadata":{"_uuid":"0e73eb23-6ac0-40f2-a603-91ab421ce084","_cell_guid":"ab404c9f-d7c7-46e5-b014-fa03280a9c16","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:33.760098Z","iopub.execute_input":"2024-12-17T03:20:33.760449Z","iopub.status.idle":"2024-12-17T03:20:33.766945Z","shell.execute_reply.started":"2024-12-17T03:20:33.760416Z","shell.execute_reply":"2024-12-17T03:20:33.765990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenizer\ntokenizer_encoder = AutoTokenizer.from_pretrained(model_name_encoder)","metadata":{"_uuid":"f91b8c3f-6e41-43a8-aa84-2fe3770ef6b2","_cell_guid":"393ea443-9a48-428e-8a47-d25c1c57ac94","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:36.795476Z","iopub.execute_input":"2024-12-17T03:20:36.795805Z","iopub.status.idle":"2024-12-17T03:20:37.348150Z","shell.execute_reply.started":"2024-12-17T03:20:36.795774Z","shell.execute_reply":"2024-12-17T03:20:37.347331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer_encoder","metadata":{"_uuid":"89ea9f37-2e4e-40b9-981f-59201349087b","_cell_guid":"1bfb11ae-567b-4087-90cc-d77deb36cb1c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:26:17.376069Z","iopub.execute_input":"2024-12-17T03:26:17.376428Z","iopub.status.idle":"2024-12-17T03:26:17.382803Z","shell.execute_reply.started":"2024-12-17T03:26:17.376396Z","shell.execute_reply":"2024-12-17T03:26:17.381778Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer_decoder = AutoTokenizer.from_pretrained(\"gpt2\")","metadata":{"_uuid":"a2631078-204e-4128-b631-71d0c0b36790","_cell_guid":"f1060f7b-e782-4d4f-89d6-e93d654c37e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:20:40.265321Z","iopub.execute_input":"2024-12-17T03:20:40.265676Z","iopub.status.idle":"2024-12-17T03:20:41.163512Z","shell.execute_reply.started":"2024-12-17T03:20:40.265646Z","shell.execute_reply":"2024-12-17T03:20:41.162824Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set padding token to eos_token for GPT-2\ntokenizer_decoder.pad_token = tokenizer_decoder.eos_token","metadata":{"_uuid":"cad8c52f-4f7a-4a39-987c-0eb7a4a84505","_cell_guid":"fccce296-a407-44b8-851d-413f14a39aab","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:20:43.185513Z","iopub.execute_input":"2024-12-17T03:20:43.185844Z","iopub.status.idle":"2024-12-17T03:20:43.190316Z","shell.execute_reply.started":"2024-12-17T03:20:43.185814Z","shell.execute_reply":"2024-12-17T03:20:43.189179Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Learnable projection layer for metacognitive profile\nprofile_projection = nn.Linear(16, context_encoder.config.hidden_size)","metadata":{"_uuid":"c5c87e4f-d68c-4daf-b658-a9b3852a5950","_cell_guid":"ebb66a87-db80-4743-b720-f6d58e5d7c59","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:46.235931Z","iopub.execute_input":"2024-12-17T03:20:46.236867Z","iopub.status.idle":"2024-12-17T03:20:46.244759Z","shell.execute_reply.started":"2024-12-17T03:20:46.236819Z","shell.execute_reply":"2024-12-17T03:20:46.243891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"profile_projection","metadata":{"_uuid":"d1451a9c-d1d4-42fa-92d6-a0401868f09b","_cell_guid":"4f8b1eee-0cef-4dae-b3b8-175f7a90690c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:20:49.345328Z","iopub.execute_input":"2024-12-17T03:20:49.345672Z","iopub.status.idle":"2024-12-17T03:20:49.350971Z","shell.execute_reply.started":"2024-12-17T03:20:49.345643Z","shell.execute_reply":"2024-12-17T03:20:49.350128Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining the PAA layers and Model","metadata":{"_uuid":"403d7dc7-4b5c-4822-86e7-fd478d88a4a6","_cell_guid":"4ba2867a-e595-4253-8f9e-1a3258358822","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class PAALayer(nn.Module):\n    def __init__(self, hidden_size):\n        super(PAALayer, self).__init__()\n        self.cross_attn = nn.MultiheadAttention(hidden_size, num_heads=8)\n        self.sigmoid = nn.Sigmoid()\n        self.linear = nn.Linear(hidden_size * 2, hidden_size)\n\n    def forward(self, persona_hidden, context_hidden, decoder_hidden, tau):\n        c1, _ = self.cross_attn(decoder_hidden, persona_hidden, persona_hidden)\n        c2, _ = self.cross_attn(decoder_hidden, context_hidden, context_hidden)\n        \n        # Adaptive weight calculation\n        w1 = self.sigmoid(self.linear(torch.cat((c1, decoder_hidden), dim=-1)))\n        w2 = 1 - w1\n\n        # Mask creation\n        m1 = torch.where(w1 > tau, 0, 1)\n        m2 = torch.where(w1 < 1 - tau, 0, 1)\n\n        # Weighted summation with masks\n        paa_output = w1 * m1 * c1 + w2 * m2 * c2 + decoder_hidden\n        return paa_output","metadata":{"_uuid":"2590c9e6-7f0c-48cf-8a04-830802fba3fa","_cell_guid":"a320e327-d213-4825-813a-73fc9d40bb41","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:54.715771Z","iopub.execute_input":"2024-12-17T03:20:54.716130Z","iopub.status.idle":"2024-12-17T03:20:54.722624Z","shell.execute_reply.started":"2024-12-17T03:20:54.716099Z","shell.execute_reply":"2024-12-17T03:20:54.721703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PAA_Model(nn.Module):\n    def __init__(self, context_encoder, decoder, profile_projection, paa_layer):\n        super(PAA_Model, self).__init__()\n        self.context_encoder = context_encoder\n        self.decoder = decoder\n        self.profile_projection = profile_projection\n        self.paa_layer = paa_layer\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, context_tokens, target_tokens, profile_vector, tau):\n        # Encode context\n        context_hidden = self.context_encoder(**context_tokens).last_hidden_state\n        print(f\"context_hidden shape: {context_hidden.shape}\")\n\n        # Project metacognitive profile\n        projected_profile = self.profile_projection(profile_vector).unsqueeze(1)\n        print(f\"projected profile shape: {projected_profile.shape}\")\n\n        # Expand persona representation\n        persona_hidden = projected_profile.expand(-1, context_hidden.size(1), -1)\n        print(f\"persona_hidden shape: {persona_hidden.shape}\")\n\n        # Resize or pad persona_hidden and context_hidden to match decoder_hidden length\n        target_length = target_tokens['input_ids'].shape[1]  # Match target sequence length (e.g., decoder_hidden length)\n        context_hidden_resized = self.resize_sequence(context_hidden, target_length)\n        persona_hidden_resized = self.resize_sequence(persona_hidden, target_length)\n\n        print(f\"Resized context_hidden shape: {context_hidden_resized.shape}\")\n        print(f\"Resized persona_hidden shape: {persona_hidden_resized.shape}\")\n\n        # Decoder's output\n        decoder_hidden = self.decoder(**target_tokens).last_hidden_state\n        print(f\"decoder_hidden shape: {decoder_hidden.shape}\")\n\n        # Apply PAA\n        paa_output = self.paa_layer(persona_hidden_resized, context_hidden_resized, decoder_hidden, tau)\n\n        # No softmax here, CrossEntropyLoss expects raw logits\n        logits = paa_output\n        target = target_tokens['input_ids'][:, 1:].contiguous().view(-1)  # Flatten target tokens\n        print(f\"logits shape: {logits.shape}\")\n        print(f\"target shape: {target.shape}\")\n\n        # Ensure logits and target have matching batch size\n        logits = logits.view(-1, logits.size(-1))  # Shape: (batch_size * seq_len, vocab_size)\n\n        assert logits.size(0) == target.size(0), f\"Batch size mismatch: logits batch size {logits.size(0)} vs target batch size {target.size(0)}\"\n\n        # Calculate loss\n        loss = self.loss_fn(logits, target)\n        return loss\n\n    def resize_sequence(self, tensor, target_length):\n        \"\"\"\n        Resize tensor sequence to the target length using padding or interpolation.\n        This method can be adjusted to use either padding or interpolation as per the requirement.\n        \"\"\"\n        current_length = tensor.size(1)\n        if current_length < target_length:\n            # Padding case\n            padding_length = target_length - current_length\n            return F.pad(tensor, (0, 0, 0, padding_length), \"constant\", 0)\n        elif current_length > target_length:\n            # Truncation case\n            return tensor[:, :target_length, :]\n        else:\n            return tensor  # No change if lengths match","metadata":{"_uuid":"a8d7c719-e84b-4a0b-9fc8-2b8a6595ff30","_cell_guid":"4d79acac-fe49-47b5-85b5-cc72e9245e08","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:20:59.780648Z","iopub.execute_input":"2024-12-17T03:20:59.781367Z","iopub.status.idle":"2024-12-17T03:20:59.790972Z","shell.execute_reply.started":"2024-12-17T03:20:59.781336Z","shell.execute_reply":"2024-12-17T03:20:59.789941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model and optimizer","metadata":{"_uuid":"6e7da9a1-cbfc-4252-b704-54fa3284718a","_cell_guid":"ec759725-103f-4caa-ba2b-053e27a761a1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"paa_layer = PAALayer(context_encoder.config.hidden_size)","metadata":{"_uuid":"9ce494c2-a4b5-4253-824f-29793100e9e5","_cell_guid":"dc990149-545b-4e00-ba99-412c1b1f959a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:21:03.265181Z","iopub.execute_input":"2024-12-17T03:21:03.265531Z","iopub.status.idle":"2024-12-17T03:21:03.295814Z","shell.execute_reply.started":"2024-12-17T03:21:03.265499Z","shell.execute_reply":"2024-12-17T03:21:03.295214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"paa_layer","metadata":{"_uuid":"329df787-7310-4ca3-93f1-cd07d724cde5","_cell_guid":"4de6dcac-7285-430b-aee9-c58f8428b141","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:21:05.705134Z","iopub.execute_input":"2024-12-17T03:21:05.705435Z","iopub.status.idle":"2024-12-17T03:21:05.710899Z","shell.execute_reply.started":"2024-12-17T03:21:05.705408Z","shell.execute_reply":"2024-12-17T03:21:05.710062Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = PAA_Model(context_encoder, decoder, profile_projection, paa_layer).to(device)","metadata":{"_uuid":"aaf3ec0f-092c-4443-b0ff-37b03e54472d","_cell_guid":"b7b44998-9108-4851-90de-97b48752cf17","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:21:26.015247Z","iopub.execute_input":"2024-12-17T03:21:26.015551Z","iopub.status.idle":"2024-12-17T03:21:26.502035Z","shell.execute_reply.started":"2024-12-17T03:21:26.015525Z","shell.execute_reply":"2024-12-17T03:21:26.501334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"_uuid":"d3b6f5ec-e141-49e9-8f63-3eab69163e68","_cell_guid":"e419196b-e304-4c73-a490-d80444e9f7e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:21:28.750644Z","iopub.execute_input":"2024-12-17T03:21:28.750929Z","iopub.status.idle":"2024-12-17T03:21:28.757993Z","shell.execute_reply.started":"2024-12-17T03:21:28.750904Z","shell.execute_reply":"2024-12-17T03:21:28.757152Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = Adam(model.parameters(), lr=5e-5)","metadata":{"_uuid":"6728cace-2cd5-484c-9286-5fd901ae92b2","_cell_guid":"8b7ca88a-639d-4beb-ac82-6d5ab5a784b1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:23:16.496291Z","iopub.execute_input":"2024-12-17T03:23:16.497046Z","iopub.status.idle":"2024-12-17T03:23:16.502307Z","shell.execute_reply.started":"2024-12-17T03:23:16.497014Z","shell.execute_reply":"2024-12-17T03:23:16.501444Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{"_uuid":"cf3edd41-4ce2-46b7-b8ba-665d2ada3230","_cell_guid":"ad28038e-14a2-4375-b356-33bce9e119c8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"file_path = \"/kaggle/input/modified-dataset/modified_dataset.csv\"\ndf = pd.read_csv(file_path)","metadata":{"_uuid":"01326d00-ef5a-402b-b16d-c0ff3e7960c7","_cell_guid":"cc4cef47-43c2-4ddd-bd8e-66597196f7a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-18T12:01:30.593006Z","iopub.execute_input":"2024-12-18T12:01:30.593834Z","iopub.status.idle":"2024-12-18T12:01:30.634112Z","shell.execute_reply.started":"2024-12-18T12:01:30.593799Z","shell.execute_reply":"2024-12-18T12:01:30.633109Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.head(5)","metadata":{"_uuid":"35193af2-5f44-4b4e-b78e-15350adc2303","_cell_guid":"4755caab-f924-4d4a-bb57-a2f840b77399","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-18T12:01:33.099883Z","iopub.execute_input":"2024-12-18T12:01:33.100221Z","iopub.status.idle":"2024-12-18T12:01:33.115930Z","shell.execute_reply.started":"2024-12-18T12:01:33.100192Z","shell.execute_reply":"2024-12-18T12:01:33.115048Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                         description  \\\n0  Create a Python program that performs the foll...   \n1  Create a Python program that accomplishes the ...   \n2  Create a Python program that accomplishes the ...   \n3  Create a Python program that accomplishes the ...   \n4  Create a Python program that accomplishes the ...   \n\n                                        student_code  \\\n0  \"\"\" store the final answer in a variable named...   \n1  \"\"\" store the final answer in a variable named...   \n2  \"\"\" store the final answer in a variable named...   \n3  x=eval(input(\"Enter your age:\"))\\ny=str(input(...   \n4  n = str(input(\"Enter your name:\"))\\na = str(in...   \n\n                                            feedback  \\\n0  [\\n    {\\n    'line_number': 2,\\n    'feedback...   \n1  [\\n    {\\n        'line_number': 4,\\n        '...   \n2  [\\n    {\\n        'line_number': 2,\\n        '...   \n3  [\\n    {\\n        'line_number': 1,\\n        '...   \n4  [\\n    {\\n        'line_number': 3,\\n        '...   \n\n                              metacognitive_feedback  \\\n0  It appears that you are almost on the right tr...   \n1  To improve your solution and better align with...   \n2  Based on your approach to the problem, it seem...   \n3  Based on your approach, it seems like you ofte...   \n4  **Metacognitive Feedback**:\\n\\nYou've made a g...   \n\n                              metacognitive_profile  \n0  [2, 1, 3, 3, 2, 3, 2, 1, 3, 1, 1, 3, 2, 1, 2, 1]  \n1  [3, 1, 2, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 2, 2, 1]  \n2  [2, 1, 1, 2, 2, 3, 3, 1, 3, 3, 2, 1, 2, 1, 2, 2]  \n3  [1, 3, 1, 3, 1, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2]  \n4  [3, 1, 3, 3, 2, 1, 3, 3, 3, 1, 2, 3, 2, 1, 1, 3]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>description</th>\n      <th>student_code</th>\n      <th>feedback</th>\n      <th>metacognitive_feedback</th>\n      <th>metacognitive_profile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Create a Python program that performs the foll...</td>\n      <td>\"\"\" store the final answer in a variable named...</td>\n      <td>[\\n    {\\n    'line_number': 2,\\n    'feedback...</td>\n      <td>It appears that you are almost on the right tr...</td>\n      <td>[2, 1, 3, 3, 2, 3, 2, 1, 3, 1, 1, 3, 2, 1, 2, 1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>\"\"\" store the final answer in a variable named...</td>\n      <td>[\\n    {\\n        'line_number': 4,\\n        '...</td>\n      <td>To improve your solution and better align with...</td>\n      <td>[3, 1, 2, 2, 3, 1, 3, 3, 2, 3, 3, 2, 1, 2, 2, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>\"\"\" store the final answer in a variable named...</td>\n      <td>[\\n    {\\n        'line_number': 2,\\n        '...</td>\n      <td>Based on your approach to the problem, it seem...</td>\n      <td>[2, 1, 1, 2, 2, 3, 3, 1, 3, 3, 2, 1, 2, 1, 2, 2]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>x=eval(input(\"Enter your age:\"))\\ny=str(input(...</td>\n      <td>[\\n    {\\n        'line_number': 1,\\n        '...</td>\n      <td>Based on your approach, it seems like you ofte...</td>\n      <td>[1, 3, 1, 3, 1, 3, 2, 3, 2, 3, 3, 2, 3, 2, 3, 2]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Create a Python program that accomplishes the ...</td>\n      <td>n = str(input(\"Enter your name:\"))\\na = str(in...</td>\n      <td>[\\n    {\\n        'line_number': 3,\\n        '...</td>\n      <td>**Metacognitive Feedback**:\\n\\nYou've made a g...</td>\n      <td>[3, 1, 3, 3, 2, 1, 3, 3, 3, 1, 2, 3, 2, 1, 1, 3]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"len(df)","metadata":{"_uuid":"16057344-5f6e-4f61-9c43-266129b302a2","_cell_guid":"1b828457-174d-4f32-83d9-df511b1211a3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:25:07.656295Z","iopub.execute_input":"2024-12-17T03:25:07.656641Z","iopub.status.idle":"2024-12-17T03:25:07.662314Z","shell.execute_reply.started":"2024-12-17T03:25:07.656610Z","shell.execute_reply":"2024-12-17T03:25:07.661287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(list(df['description']))","metadata":{"_uuid":"d646c61f-941a-4e1f-a6de-5f345f0af03b","_cell_guid":"152d555b-f34f-4b99-8b07-a45690325097","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:25:44.656848Z","iopub.execute_input":"2024-12-17T03:25:44.657666Z","iopub.status.idle":"2024-12-17T03:25:44.662746Z","shell.execute_reply.started":"2024-12-17T03:25:44.657628Z","shell.execute_reply":"2024-12-17T03:25:44.661923Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\ndef tokenize_data(df, tokenizer_encoder,tokenizer_decoder):\n    context_tokens = tokenizer_encoder(list(df['description']), padding=True, truncation=True, return_tensors=\"pt\")\n    target_tokens = tokenizer_decoder(list(df['metacognitive_feedback']), padding=True, truncation=True, return_tensors=\"pt\")\n    profile_vectors = torch.tensor([ast.literal_eval(profile) for profile in df['metacognitive_profile']], dtype=torch.float)\n    return context_tokens, target_tokens, profile_vectors","metadata":{"_uuid":"fed6a7a7-d6a0-4a9a-a803-530938343446","_cell_guid":"7fdbe31b-907e-4ae7-abdc-d8b0a1a1dcc0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:23:31.896044Z","iopub.execute_input":"2024-12-17T03:23:31.896666Z","iopub.status.idle":"2024-12-17T03:23:31.901631Z","shell.execute_reply.started":"2024-12-17T03:23:31.896632Z","shell.execute_reply":"2024-12-17T03:23:31.900687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context_tokens, target_tokens, profile_vectors = tokenize_data(df, tokenizer_encoder,tokenizer_decoder)","metadata":{"_uuid":"a44c727f-722b-46dd-9874-2bd7b277ee27","_cell_guid":"502a6455-0e9a-4d8b-89eb-bb75a612ce88","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:23:35.760810Z","iopub.execute_input":"2024-12-17T03:23:35.761670Z","iopub.status.idle":"2024-12-17T03:23:36.180240Z","shell.execute_reply.started":"2024-12-17T03:23:35.761631Z","shell.execute_reply":"2024-12-17T03:23:36.179516Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(target_tokens)","metadata":{"_uuid":"0df4ff65-67d0-4367-a393-17b564249cf2","_cell_guid":"99140815-f0ba-4cf8-a945-18d38bbea3b6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:24:22.895921Z","iopub.execute_input":"2024-12-17T03:24:22.896682Z","iopub.status.idle":"2024-12-17T03:24:22.902789Z","shell.execute_reply.started":"2024-12-17T03:24:22.896624Z","shell.execute_reply":"2024-12-17T03:24:22.901713Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"profile_vectors","metadata":{"_uuid":"d5511411-5da8-4e31-b9d0-be4b17b723b7","_cell_guid":"4fc1564d-1e14-4e6a-b2c3-d53d278b40c3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:24:37.355597Z","iopub.execute_input":"2024-12-17T03:24:37.355935Z","iopub.status.idle":"2024-12-17T03:24:37.386198Z","shell.execute_reply.started":"2024-12-17T03:24:37.355905Z","shell.execute_reply":"2024-12-17T03:24:37.385211Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context_tokens = {key: value.to(device) for key, value in context_tokens.items()}\ntarget_tokens = {key: value.to(device) for key, value in target_tokens.items()}\nprofile_vectors = profile_vectors.to(device)","metadata":{"_uuid":"02b04bb3-4dac-4d9d-bedf-93c7e33a65be","_cell_guid":"48d3d450-8fdc-48de-8d99-05587d0d3eac","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T03:26:46.460774Z","iopub.execute_input":"2024-12-17T03:26:46.461436Z","iopub.status.idle":"2024-12-17T03:26:46.482051Z","shell.execute_reply.started":"2024-12-17T03:26:46.461397Z","shell.execute_reply":"2024-12-17T03:26:46.481185Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataLoader\ntrain_data = TensorDataset(context_tokens['input_ids'], target_tokens['input_ids'], profile_vectors)\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True)","metadata":{"_uuid":"074a07c6-02f9-44e0-8c64-967ca78f8250","_cell_guid":"d655b657-635d-42b9-9c99-8ef9c7c7af02","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:26:54.280836Z","iopub.execute_input":"2024-12-17T03:26:54.281154Z","iopub.status.idle":"2024-12-17T03:26:54.285616Z","shell.execute_reply.started":"2024-12-17T03:26:54.281127Z","shell.execute_reply":"2024-12-17T03:26:54.284698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training loop","metadata":{"_uuid":"94e3aace-7107-4a82-a83f-e6403a3ad4c7","_cell_guid":"a1b089b5-2fdf-4b22-ac39-fe4f7d0d368a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model.train()\nnum_epochs = 5\ntau = 0.5\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for context_ids, target_ids, profile_vector in train_loader:\n        # Move tensors to the GPU (if available)\n        context_ids = context_ids.to(device)\n        target_ids = target_ids.to(device)\n        profile_vector = profile_vector.to(device)\n\n        optimizer.zero_grad()\n\n        # Prepare input tensors\n        context_tokens = {'input_ids': context_ids, 'attention_mask': context_ids != tokenizer_encoder.pad_token_id}\n        target_tokens = {'input_ids': target_ids, 'attention_mask': target_ids != tokenizer_decoder.pad_token_id}\n\n        # Forward pass through the model\n        loss = model(context_tokens, target_tokens, profile_vector, tau)\n\n        # Backpropagate and update the model\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")","metadata":{"_uuid":"bf476f50-8f76-4429-afd0-2f47630fa9dc","_cell_guid":"2347f634-be12-45b1-8d79-d1f0bb114c8a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-17T03:26:58.135783Z","iopub.execute_input":"2024-12-17T03:26:58.136128Z","iopub.status.idle":"2024-12-17T03:27:00.030683Z","shell.execute_reply.started":"2024-12-17T03:26:58.136098Z","shell.execute_reply":"2024-12-17T03:27:00.029527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"_uuid":"7a922bc4-5777-4a1c-9e5a-bd820b02e268","_cell_guid":"e6548ec5-b73a-48ec-832e-c1865f3b9fb3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-16T17:07:38.511418Z","iopub.execute_input":"2024-12-16T17:07:38.512324Z","iopub.status.idle":"2024-12-16T17:07:38.517039Z","shell.execute_reply.started":"2024-12-16T17:07:38.512265Z","shell.execute_reply":"2024-12-16T17:07:38.516219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.memory_summary())","metadata":{"_uuid":"ec7b2129-3b55-4f70-985c-b764d7c43a0e","_cell_guid":"c57c0ba7-bba8-4ebc-9c2e-ef5ad6ca52c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-16T17:11:28.010818Z","iopub.execute_input":"2024-12-16T17:11:28.011421Z","iopub.status.idle":"2024-12-16T17:11:28.016583Z","shell.execute_reply.started":"2024-12-16T17:11:28.011387Z","shell.execute_reply":"2024-12-16T17:11:28.015637Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Context Encoder","metadata":{"_uuid":"a54f4e1e-7bac-4e3e-9456-28a9f70ef0c0","_cell_guid":"acd7f754-97d2-472f-8cd8-8a17691e400d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"encoder = BertModel.from_pretrained(model_name_encoder)","metadata":{"_uuid":"a39c6371-5059-45b5-9ab1-f028d7456565","_cell_guid":"06d0cb18-5076-4128-aa7d-4ebeb504be81","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-18T10:46:56.646681Z","iopub.execute_input":"2024-12-18T10:46:56.647523Z","iopub.status.idle":"2024-12-18T10:46:57.618904Z","shell.execute_reply.started":"2024-12-18T10:46:56.647486Z","shell.execute_reply":"2024-12-18T10:46:57.618227Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tokenizer_encoder = BertTokenizer.from_pretrained(model_name_encoder)","metadata":{"_uuid":"a89927c0-4e9a-47aa-a9dd-fcc6bb02f3b1","_cell_guid":"10fd9213-45db-4d6a-a89c-6231690ce158","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T10:46:59.408146Z","iopub.execute_input":"2024-12-18T10:46:59.408490Z","iopub.status.idle":"2024-12-18T10:46:59.674064Z","shell.execute_reply.started":"2024-12-18T10:46:59.408464Z","shell.execute_reply":"2024-12-18T10:46:59.673031Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Function to encode context\ndef encode_context(description, student_code, feedback, max_len=512):\n    context = f\"Description: {description} Student Code: {student_code} Feedback: {feedback}\"\n    inputs = tokenizer_encoder(\n        context, \n        padding='max_length', \n        truncation=True, \n        max_length=max_len, \n        return_tensors='pt'\n    )\n    return inputs","metadata":{"_uuid":"41b25e5d-60fe-4bb1-b89c-bbeed9ff13ee","_cell_guid":"92a5215c-abaa-445b-a0ac-278dfd56b1a1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T06:10:12.826392Z","iopub.execute_input":"2024-12-18T06:10:12.826687Z","iopub.status.idle":"2024-12-18T06:10:12.831427Z","shell.execute_reply.started":"2024-12-18T06:10:12.826659Z","shell.execute_reply":"2024-12-18T06:10:12.830537Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"encoded_context = []","metadata":{"_uuid":"ce0f4149-0c29-4f74-ba75-aa50b1564395","_cell_guid":"e53555c7-178b-47af-87b8-53707eb22005","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T06:10:12.833144Z","iopub.execute_input":"2024-12-18T06:10:12.833442Z","iopub.status.idle":"2024-12-18T06:10:12.847475Z","shell.execute_reply.started":"2024-12-18T06:10:12.833397Z","shell.execute_reply":"2024-12-18T06:10:12.846658Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"for idx, row in df.iterrows():\n    description = row['description']\n    student_code = row['student_code']\n    feedback = row['feedback']\n    \n    # Encode context (IU)\n    context_inputs = encode_context(student_code, feedback, description)\n\n    # Get BERT hidden states\n    with torch.no_grad():\n        hU = encoder(**context_inputs).last_hidden_state  # Shape: (1, max_len, 768)\n\n    encoded_context.append(hU.squeeze(0))\n    print(len(encoded_context))","metadata":{"_uuid":"baa2843a-52f7-43fd-8aac-d54f8a28d8c5","_cell_guid":"05dee144-6050-48e0-ae36-94902555b9ca","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:23:49.835039Z","iopub.execute_input":"2024-12-17T07:23:49.835345Z","iopub.status.idle":"2024-12-17T07:26:42.829537Z","shell.execute_reply.started":"2024-12-17T07:23:49.835317Z","shell.execute_reply":"2024-12-17T07:26:42.828658Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Encoded Context Shape: {encoded_context[0].shape}\")","metadata":{"_uuid":"adb19207-b959-45dc-8128-13c6bd21c436","_cell_guid":"ec4ca9f1-48a4-4d45-9403-eeadd1dbcd47","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:29:02.920115Z","iopub.execute_input":"2024-12-17T07:29:02.920960Z","iopub.status.idle":"2024-12-17T07:29:02.925332Z","shell.execute_reply.started":"2024-12-17T07:29:02.920926Z","shell.execute_reply":"2024-12-17T07:29:02.924286Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_context[0]","metadata":{"_uuid":"d7098c76-9110-4f46-8f58-0aa1c5ee0fe9","_cell_guid":"9a0f8b1a-f1c1-40e5-a6b9-969b0763a0c9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T07:29:29.144538Z","iopub.execute_input":"2024-12-17T07:29:29.145175Z","iopub.status.idle":"2024-12-17T07:29:29.212816Z","shell.execute_reply.started":"2024-12-17T07:29:29.145144Z","shell.execute_reply":"2024-12-17T07:29:29.212099Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ContextEncoder(nn.Module):\n    def __init__(self, bert_model_name='bert-base-uncased', output_dim=768):\n        super(ContextEncoder, self).__init__()\n        \n        # BERT model for encoding the context (student_code, feedback, description)\n        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n        self.bert_encoder = BertModel.from_pretrained(bert_model_name)\n        \n        # Optional: Linear layer to project BERT output to the desired dimension\n        self.fc = nn.Linear(self.bert_encoder.config.hidden_size, output_dim)\n\n    def forward(self, description, student_code, feedback):\n        # Combine the context information (description, student_code, feedback)\n        context = f\"Description: {description} Student Code: {student_code} Feedback: {feedback}\"\n        \n        # Tokenize the input context and prepare it for BERT\n        encoded_inputs = self.tokenizer(\n            context, \n            return_tensors='pt', \n            padding='max_length', \n            truncation=True, \n            max_length=512\n        )\n        \n        # Encode the context using BERT\n        with torch.no_grad():\n            context_hidden_states = self.bert_encoder(**encoded_inputs).last_hidden_state  # (batch_size, seq_len, hidden_dim)\n        \n        # Take the mean of the hidden states across the sequence length to get a fixed-size representation\n        context_rep = context_hidden_states.mean(dim=1)  # (batch_size, hidden_dim)\n        \n        context_rep = self.fc(context_rep)  # (batch_size, output_dim)\n        final_rep = context_rep.unsqueeze(1)\n        \n        return final_rep","metadata":{"_uuid":"f74b60da-0791-433b-8273-8bbc2e262947","_cell_guid":"a0554a5d-0cc3-42b0-afe9-b3605296e57b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T12:01:57.419694Z","iopub.execute_input":"2024-12-18T12:01:57.420543Z","iopub.status.idle":"2024-12-18T12:01:57.429969Z","shell.execute_reply.started":"2024-12-18T12:01:57.420491Z","shell.execute_reply":"2024-12-18T12:01:57.429125Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"context_encoder = ContextEncoder()","metadata":{"_uuid":"4c808304-de9e-434f-8b57-1d795efcf084","_cell_guid":"7ac1b345-eb9a-43db-86b2-90aa33d65e6e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T12:01:59.627642Z","iopub.execute_input":"2024-12-18T12:01:59.628401Z","iopub.status.idle":"2024-12-18T12:02:10.273284Z","shell.execute_reply.started":"2024-12-18T12:01:59.628370Z","shell.execute_reply":"2024-12-18T12:02:10.272633Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"943fbcb494894dd5a6166b88b22d7b0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b37eea7a50da40b28d71bd1b965d8383"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b530000e87054da1b240f0ddbb494060"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b4f3e5aa66b465da3760ba141a8b230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"173aede57c404a4ba160b9791f362857"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"context=[]","metadata":{"_uuid":"dafbf2d2-65d0-4960-8db4-8549dce8dabb","_cell_guid":"95056cd7-603c-4170-84c5-9e517139add4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T10:18:50.034198Z","iopub.execute_input":"2024-12-18T10:18:50.034990Z","iopub.status.idle":"2024-12-18T10:18:50.038840Z","shell.execute_reply.started":"2024-12-18T10:18:50.034943Z","shell.execute_reply":"2024-12-18T10:18:50.038017Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"for idx, row in df.iterrows():\n    description = row['description']\n    student_code = row['student_code']\n    feedback = row['feedback']\n    \n    # Get the context representation by passing the description, student_code, and feedback\n    context_rep = context_encoder(description, student_code, feedback)\n    \n    # Append the result to the encoded context list\n    context.append(context_rep)\n    print(len(context))","metadata":{"_uuid":"e56e7167-679e-4349-9964-71fe45a20438","_cell_guid":"3f9e3a8a-5ba8-4e28-98f4-7478876313b9","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2024-12-18T10:18:51.720764Z","iopub.execute_input":"2024-12-18T10:18:51.721134Z","iopub.status.idle":"2024-12-18T10:18:55.177095Z","shell.execute_reply.started":"2024-12-18T10:18:51.721102Z","shell.execute_reply":"2024-12-18T10:18:55.175581Z"},"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"1\n2\n3\n4\n5\n6\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[93], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m feedback \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeedback\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get the context representation by passing the description, student_code, and feedback\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m context_rep \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeedback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Append the result to the encoded context list\u001b[39;00m\n\u001b[1;32m     10\u001b[0m context\u001b[38;5;241m.\u001b[39mappend(context_rep)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[90], line 27\u001b[0m, in \u001b[0;36mContextEncoder.forward\u001b[0;34m(self, description, student_code, feedback)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Encode the context using BERT\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 27\u001b[0m     context_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# (batch_size, seq_len, hidden_dim)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Take the mean of the hidden states across the sequence length to get a fixed-size representation\u001b[39;00m\n\u001b[1;32m     30\u001b[0m context_rep \u001b[38;5;241m=\u001b[39m context_hidden_states\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, hidden_dim)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:409\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states))\n\u001b[0;32m--> 409\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n\u001b[1;32m    411\u001b[0m         key_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":93},{"cell_type":"code","source":"print(context[0].shape)","metadata":{"_uuid":"bf333806-c81f-45ee-b5ba-7d19dafdeb5e","_cell_guid":"f17a0c4a-03e8-4a08-afab-421d2aa48612","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T10:18:58.490407Z","iopub.execute_input":"2024-12-18T10:18:58.491238Z","iopub.status.idle":"2024-12-18T10:18:58.495708Z","shell.execute_reply.started":"2024-12-18T10:18:58.491184Z","shell.execute_reply":"2024-12-18T10:18:58.494753Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"torch.Size([1, 1, 768])\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"context[0]","metadata":{"_uuid":"f38b8ebf-bf7d-4f7b-a626-64088718109c","_cell_guid":"9cbab324-f885-4773-b26b-db511b8df145","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:04:07.314666Z","iopub.execute_input":"2024-12-17T08:04:07.315591Z","iopub.status.idle":"2024-12-17T08:04:07.326179Z","shell.execute_reply.started":"2024-12-17T08:04:07.315554Z","shell.execute_reply":"2024-12-17T08:04:07.325321Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Persona Encoder","metadata":{"_uuid":"c361dd0d-650e-476f-ba5e-7e2d0b828f5a","_cell_guid":"dc447f6d-ee00-42b3-b5cf-ad411c982f33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class PersonaEncoder(nn.Module):\n    def __init__(self, metacognitive_dim=16, output_dim=768):\n        super(PersonaEncoder, self).__init__()\n        \n        # Linear layer to map the metacognitive vector to the output dimension\n        self.metacognitive_fc = nn.Linear(metacognitive_dim, output_dim)  # from 16 to 768\n        \n        # Final projection layer to further process the output\n        self.final_fc = nn.Linear(output_dim, output_dim)\n\n    def forward(self, metacognitive_vector):\n        # Process the metacognitive vector through the linear layer\n        metacognitive_rep = self.metacognitive_fc(metacognitive_vector)  # (batch_size, output_dim)\n        \n        # Optionally, apply another linear layer or activation if necessary\n        final_rep = self.final_fc(metacognitive_rep)  # (batch_size, output_dim)\n        persona_rep = final_rep.unsqueeze(1)\n        \n        return persona_rep","metadata":{"_uuid":"223b0a76-4222-40ab-bc06-b0fb4508f33e","_cell_guid":"be7a4518-16cf-4c13-bc91-36dbdca41905","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T12:02:10.274579Z","iopub.execute_input":"2024-12-18T12:02:10.274840Z","iopub.status.idle":"2024-12-18T12:02:10.280045Z","shell.execute_reply.started":"2024-12-18T12:02:10.274815Z","shell.execute_reply":"2024-12-18T12:02:10.279244Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"persona_encoder = PersonaEncoder()","metadata":{"_uuid":"2b06f448-b5eb-478b-ad21-7d131941f112","_cell_guid":"3a73de3f-bb58-4b0f-b60a-139c010d8769","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T12:02:10.281239Z","iopub.execute_input":"2024-12-18T12:02:10.282232Z","iopub.status.idle":"2024-12-18T12:02:10.298221Z","shell.execute_reply.started":"2024-12-18T12:02:10.282187Z","shell.execute_reply":"2024-12-18T12:02:10.297573Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"encoded_persona=[]","metadata":{"_uuid":"b6141891-7c7b-4341-b2b0-96aa8cccde8f","_cell_guid":"a94c268f-1ae0-4f1e-bbc2-8b6293ba70e5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T10:19:51.773106Z","iopub.execute_input":"2024-12-18T10:19:51.773693Z","iopub.status.idle":"2024-12-18T10:19:51.777522Z","shell.execute_reply.started":"2024-12-18T10:19:51.773664Z","shell.execute_reply":"2024-12-18T10:19:51.776690Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"import ast\nfor idx, row in df.iterrows():\n        # Convert the string representation of the list to an actual list of integers\n        #metacognitive_vector = ast.literal_eval(row['metacognitive_profile'])\n        metacognitive_tensor = torch.tensor([ast.literal_eval(profile) for profile in df['metacognitive_profile']], dtype=torch.float)\n        \n        # Convert to a PyTorch tensor of shape (1, 16)\n        #metacognitive_tensor = torch.tensor(metacognitive_vector, dtype=torch.float32).unsqueeze(0)\n        \n        # Encode using the PersonaEncoder\n        persona_rep = persona_encoder(metacognitive_tensor)\n        \n        # Append result to the list\n        encoded_persona.append(persona_rep)\n        print(f\"Encoded persona {idx+1}/{len(df)}\")","metadata":{"_uuid":"95cb66ff-8644-4921-b0ef-bbcf389bf474","_cell_guid":"e1942726-3cc6-4c84-9de7-c48e85378431","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2024-12-18T10:19:52.561691Z","iopub.execute_input":"2024-12-18T10:19:52.562449Z","iopub.status.idle":"2024-12-18T10:19:57.347847Z","shell.execute_reply.started":"2024-12-18T10:19:52.562418Z","shell.execute_reply":"2024-12-18T10:19:57.346897Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Encoded persona 1/366\nEncoded persona 2/366\nEncoded persona 3/366\nEncoded persona 4/366\nEncoded persona 5/366\nEncoded persona 6/366\nEncoded persona 7/366\nEncoded persona 8/366\nEncoded persona 9/366\nEncoded persona 10/366\nEncoded persona 11/366\nEncoded persona 12/366\nEncoded persona 13/366\nEncoded persona 14/366\nEncoded persona 15/366\nEncoded persona 16/366\nEncoded persona 17/366\nEncoded persona 18/366\nEncoded persona 19/366\nEncoded persona 20/366\nEncoded persona 21/366\nEncoded persona 22/366\nEncoded persona 23/366\nEncoded persona 24/366\nEncoded persona 25/366\nEncoded persona 26/366\nEncoded persona 27/366\nEncoded persona 28/366\nEncoded persona 29/366\nEncoded persona 30/366\nEncoded persona 31/366\nEncoded persona 32/366\nEncoded persona 33/366\nEncoded persona 34/366\nEncoded persona 35/366\nEncoded persona 36/366\nEncoded persona 37/366\nEncoded persona 38/366\nEncoded persona 39/366\nEncoded persona 40/366\nEncoded persona 41/366\nEncoded persona 42/366\nEncoded persona 43/366\nEncoded persona 44/366\nEncoded persona 45/366\nEncoded persona 46/366\nEncoded persona 47/366\nEncoded persona 48/366\nEncoded persona 49/366\nEncoded persona 50/366\nEncoded persona 51/366\nEncoded persona 52/366\nEncoded persona 53/366\nEncoded persona 54/366\nEncoded persona 55/366\nEncoded persona 56/366\nEncoded persona 57/366\nEncoded persona 58/366\nEncoded persona 59/366\nEncoded persona 60/366\nEncoded persona 61/366\nEncoded persona 62/366\nEncoded persona 63/366\nEncoded persona 64/366\nEncoded persona 65/366\nEncoded persona 66/366\nEncoded persona 67/366\nEncoded persona 68/366\nEncoded persona 69/366\nEncoded persona 70/366\nEncoded persona 71/366\nEncoded persona 72/366\nEncoded persona 73/366\nEncoded persona 74/366\nEncoded persona 75/366\nEncoded persona 76/366\nEncoded persona 77/366\nEncoded persona 78/366\nEncoded persona 79/366\nEncoded persona 80/366\nEncoded persona 81/366\nEncoded persona 82/366\nEncoded persona 83/366\nEncoded persona 84/366\nEncoded persona 85/366\nEncoded persona 86/366\nEncoded persona 87/366\nEncoded persona 88/366\nEncoded persona 89/366\nEncoded persona 90/366\nEncoded persona 91/366\nEncoded persona 92/366\nEncoded persona 93/366\nEncoded persona 94/366\nEncoded persona 95/366\nEncoded persona 96/366\nEncoded persona 97/366\nEncoded persona 98/366\nEncoded persona 99/366\nEncoded persona 100/366\nEncoded persona 101/366\nEncoded persona 102/366\nEncoded persona 103/366\nEncoded persona 104/366\nEncoded persona 105/366\nEncoded persona 106/366\nEncoded persona 107/366\nEncoded persona 108/366\nEncoded persona 109/366\nEncoded persona 110/366\nEncoded persona 111/366\nEncoded persona 112/366\nEncoded persona 113/366\nEncoded persona 114/366\nEncoded persona 115/366\nEncoded persona 116/366\nEncoded persona 117/366\nEncoded persona 118/366\nEncoded persona 119/366\nEncoded persona 120/366\nEncoded persona 121/366\nEncoded persona 122/366\nEncoded persona 123/366\nEncoded persona 124/366\nEncoded persona 125/366\nEncoded persona 126/366\nEncoded persona 127/366\nEncoded persona 128/366\nEncoded persona 129/366\nEncoded persona 130/366\nEncoded persona 131/366\nEncoded persona 132/366\nEncoded persona 133/366\nEncoded persona 134/366\nEncoded persona 135/366\nEncoded persona 136/366\nEncoded persona 137/366\nEncoded persona 138/366\nEncoded persona 139/366\nEncoded persona 140/366\nEncoded persona 141/366\nEncoded persona 142/366\nEncoded persona 143/366\nEncoded persona 144/366\nEncoded persona 145/366\nEncoded persona 146/366\nEncoded persona 147/366\nEncoded persona 148/366\nEncoded persona 149/366\nEncoded persona 150/366\nEncoded persona 151/366\nEncoded persona 152/366\nEncoded persona 153/366\nEncoded persona 154/366\nEncoded persona 155/366\nEncoded persona 156/366\nEncoded persona 157/366\nEncoded persona 158/366\nEncoded persona 159/366\nEncoded persona 160/366\nEncoded persona 161/366\nEncoded persona 162/366\nEncoded persona 163/366\nEncoded persona 164/366\nEncoded persona 165/366\nEncoded persona 166/366\nEncoded persona 167/366\nEncoded persona 168/366\nEncoded persona 169/366\nEncoded persona 170/366\nEncoded persona 171/366\nEncoded persona 172/366\nEncoded persona 173/366\nEncoded persona 174/366\nEncoded persona 175/366\nEncoded persona 176/366\nEncoded persona 177/366\nEncoded persona 178/366\nEncoded persona 179/366\nEncoded persona 180/366\nEncoded persona 181/366\nEncoded persona 182/366\nEncoded persona 183/366\nEncoded persona 184/366\nEncoded persona 185/366\nEncoded persona 186/366\nEncoded persona 187/366\nEncoded persona 188/366\nEncoded persona 189/366\nEncoded persona 190/366\nEncoded persona 191/366\nEncoded persona 192/366\nEncoded persona 193/366\nEncoded persona 194/366\nEncoded persona 195/366\nEncoded persona 196/366\nEncoded persona 197/366\nEncoded persona 198/366\nEncoded persona 199/366\nEncoded persona 200/366\nEncoded persona 201/366\nEncoded persona 202/366\nEncoded persona 203/366\nEncoded persona 204/366\nEncoded persona 205/366\nEncoded persona 206/366\nEncoded persona 207/366\nEncoded persona 208/366\nEncoded persona 209/366\nEncoded persona 210/366\nEncoded persona 211/366\nEncoded persona 212/366\nEncoded persona 213/366\nEncoded persona 214/366\nEncoded persona 215/366\nEncoded persona 216/366\nEncoded persona 217/366\nEncoded persona 218/366\nEncoded persona 219/366\nEncoded persona 220/366\nEncoded persona 221/366\nEncoded persona 222/366\nEncoded persona 223/366\nEncoded persona 224/366\nEncoded persona 225/366\nEncoded persona 226/366\nEncoded persona 227/366\nEncoded persona 228/366\nEncoded persona 229/366\nEncoded persona 230/366\nEncoded persona 231/366\nEncoded persona 232/366\nEncoded persona 233/366\nEncoded persona 234/366\nEncoded persona 235/366\nEncoded persona 236/366\nEncoded persona 237/366\nEncoded persona 238/366\nEncoded persona 239/366\nEncoded persona 240/366\nEncoded persona 241/366\nEncoded persona 242/366\nEncoded persona 243/366\nEncoded persona 244/366\nEncoded persona 245/366\nEncoded persona 246/366\nEncoded persona 247/366\nEncoded persona 248/366\nEncoded persona 249/366\nEncoded persona 250/366\nEncoded persona 251/366\nEncoded persona 252/366\nEncoded persona 253/366\nEncoded persona 254/366\nEncoded persona 255/366\nEncoded persona 256/366\nEncoded persona 257/366\nEncoded persona 258/366\nEncoded persona 259/366\nEncoded persona 260/366\nEncoded persona 261/366\nEncoded persona 262/366\nEncoded persona 263/366\nEncoded persona 264/366\nEncoded persona 265/366\nEncoded persona 266/366\nEncoded persona 267/366\nEncoded persona 268/366\nEncoded persona 269/366\nEncoded persona 270/366\nEncoded persona 271/366\nEncoded persona 272/366\nEncoded persona 273/366\nEncoded persona 274/366\nEncoded persona 275/366\nEncoded persona 276/366\nEncoded persona 277/366\nEncoded persona 278/366\nEncoded persona 279/366\nEncoded persona 280/366\nEncoded persona 281/366\nEncoded persona 282/366\nEncoded persona 283/366\nEncoded persona 284/366\nEncoded persona 285/366\nEncoded persona 286/366\nEncoded persona 287/366\nEncoded persona 288/366\nEncoded persona 289/366\nEncoded persona 290/366\nEncoded persona 291/366\nEncoded persona 292/366\nEncoded persona 293/366\nEncoded persona 294/366\nEncoded persona 295/366\nEncoded persona 296/366\nEncoded persona 297/366\nEncoded persona 298/366\nEncoded persona 299/366\nEncoded persona 300/366\nEncoded persona 301/366\nEncoded persona 302/366\nEncoded persona 303/366\nEncoded persona 304/366\nEncoded persona 305/366\nEncoded persona 306/366\nEncoded persona 307/366\nEncoded persona 308/366\nEncoded persona 309/366\nEncoded persona 310/366\nEncoded persona 311/366\nEncoded persona 312/366\nEncoded persona 313/366\nEncoded persona 314/366\nEncoded persona 315/366\nEncoded persona 316/366\nEncoded persona 317/366\nEncoded persona 318/366\nEncoded persona 319/366\nEncoded persona 320/366\nEncoded persona 321/366\nEncoded persona 322/366\nEncoded persona 323/366\nEncoded persona 324/366\nEncoded persona 325/366\nEncoded persona 326/366\nEncoded persona 327/366\nEncoded persona 328/366\nEncoded persona 329/366\nEncoded persona 330/366\nEncoded persona 331/366\nEncoded persona 332/366\nEncoded persona 333/366\nEncoded persona 334/366\nEncoded persona 335/366\nEncoded persona 336/366\nEncoded persona 337/366\nEncoded persona 338/366\nEncoded persona 339/366\nEncoded persona 340/366\nEncoded persona 341/366\nEncoded persona 342/366\nEncoded persona 343/366\nEncoded persona 344/366\nEncoded persona 345/366\nEncoded persona 346/366\nEncoded persona 347/366\nEncoded persona 348/366\nEncoded persona 349/366\nEncoded persona 350/366\nEncoded persona 351/366\nEncoded persona 352/366\nEncoded persona 353/366\nEncoded persona 354/366\nEncoded persona 355/366\nEncoded persona 356/366\nEncoded persona 357/366\nEncoded persona 358/366\nEncoded persona 359/366\nEncoded persona 360/366\nEncoded persona 361/366\nEncoded persona 362/366\nEncoded persona 363/366\nEncoded persona 364/366\nEncoded persona 365/366\nEncoded persona 366/366\n","output_type":"stream"}],"execution_count":102},{"cell_type":"code","source":"encoded_persona[0].shape","metadata":{"_uuid":"6557deba-b0b1-4234-95fe-5d72fcb9ecb7","_cell_guid":"ebc0d3ff-25d6-4bd6-8ccb-e025ed999891","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T10:20:00.694177Z","iopub.execute_input":"2024-12-18T10:20:00.694512Z","iopub.status.idle":"2024-12-18T10:20:00.700193Z","shell.execute_reply.started":"2024-12-18T10:20:00.694482Z","shell.execute_reply":"2024-12-18T10:20:00.699157Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"torch.Size([366, 1, 768])"},"metadata":{}}],"execution_count":103},{"cell_type":"code","source":"encoded_persona[365]","metadata":{"_uuid":"f2436afd-1a2e-4d55-ad22-120412ab404b","_cell_guid":"294930e9-61c7-4e4b-aca3-1670267381fa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:04:50.395796Z","iopub.execute_input":"2024-12-17T08:04:50.396482Z","iopub.status.idle":"2024-12-17T08:04:50.403762Z","shell.execute_reply.started":"2024-12-17T08:04:50.396447Z","shell.execute_reply":"2024-12-17T08:04:50.402909Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(encoded_persona)","metadata":{"_uuid":"54304fe4-1ac4-4b49-a40e-74f31032d9bb","_cell_guid":"52ebe085-8006-41e2-ad60-f2a09a2df698","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-17T08:04:38.230904Z","iopub.execute_input":"2024-12-17T08:04:38.231243Z","iopub.status.idle":"2024-12-17T08:04:38.236596Z","shell.execute_reply.started":"2024-12-17T08:04:38.231210Z","shell.execute_reply":"2024-12-17T08:04:38.235614Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Stack context and persona encodings into tensors\n#encoded_context_tensor = torch.cat(encoded_context, dim=0)  # Shape: [N, 768]\nencoded_persona_tensor = torch.stack(encoded_persona, dim=0)  # Shape: [M, 768]","metadata":{"_uuid":"246e78f1-1385-4913-9bef-cdba468b436a","_cell_guid":"02207754-99c9-42c0-95c3-a60afcbe9d9f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T10:20:13.410827Z","iopub.execute_input":"2024-12-18T10:20:13.411201Z","iopub.status.idle":"2024-12-18T10:20:13.583822Z","shell.execute_reply.started":"2024-12-18T10:20:13.411167Z","shell.execute_reply":"2024-12-18T10:20:13.582808Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"print(\"the encoded context tensor dimensions:\",encoded_context_tensor.shape)","metadata":{"_uuid":"a629c5c9-ceed-466e-bdea-9c9afe4a2388","_cell_guid":"cfbf5ca5-b31b-4f46-8e61-15876d0b4e01","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T10:20:15.572320Z","iopub.execute_input":"2024-12-18T10:20:15.572660Z","iopub.status.idle":"2024-12-18T10:20:15.605889Z","shell.execute_reply.started":"2024-12-18T10:20:15.572631Z","shell.execute_reply":"2024-12-18T10:20:15.604902Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe encoded context tensor dimensions:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mencoded_context_tensor\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n","\u001b[0;31mNameError\u001b[0m: name 'encoded_context_tensor' is not defined"],"ename":"NameError","evalue":"name 'encoded_context_tensor' is not defined","output_type":"error"}],"execution_count":105},{"cell_type":"code","source":"print(\"the encoded persona tensor dimensions:\",encoded_persona_tensor.shape)","metadata":{"_uuid":"80d6c22d-7b2e-4158-b4db-31f4ad1d707f","_cell_guid":"72570788-ab31-4285-aafa-77079b364eae","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-18T10:20:19.067667Z","iopub.execute_input":"2024-12-18T10:20:19.068039Z","iopub.status.idle":"2024-12-18T10:20:19.072923Z","shell.execute_reply.started":"2024-12-18T10:20:19.068006Z","shell.execute_reply":"2024-12-18T10:20:19.072001Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"the encoded persona tensor dimensions: torch.Size([366, 366, 1, 768])\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"","metadata":{"_uuid":"71d4f913-ea96-4832-a7f9-100ca1640c91","_cell_guid":"7331676f-d099-43ce-aed8-1eab6262ecd2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PAA Layer","metadata":{"_uuid":"04a41811-8b4d-43e4-9be4-7594ab95dc60","_cell_guid":"f3198c17-0559-4904-9269-17c05a196160","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"* hR : Slef attention obtained by the metacognitive reposnse + metacognitive vector\n* oP : cross attention result from (hR, hP)\n* oC : cross attention result from (hR, hC)\n* persona_length : metacognitive vector encoded tensor length\n* context_length : context(description + student code + general feedback) encoded tensor length","metadata":{}},{"cell_type":"code","source":"class PAALayer(nn.Module):\n    def __init__(self, hidden_dimension = 768 , tau=0.5):\n        super(PAALayer, self).__init__()\n        self.hidden_dimenstion = hidden_dimension\n        self.tau = tau\n        \n        # Linear layers for generating the initial weights (wpersona)\n        self.fc = nn.Linear(2 * hidden_dimension, hidden_dimension)  # Concatenated hR and oP\n        self.sigmoid = nn.Sigmoid()\n\n\n    def forward(self, hR , oP, oC):\n        #cross attention results\n        Mp_input  = torch.cat([hR,oP], dim=-1)#1\n        print(\"Mp input:\", Mp_input.shape)\n        Mp = self.fc(Mp_input)#2\n        print(\"Mp:\",Mp.shape)\n        Wp = self.sigmoid(Mp) #3\n        print(\"Wp:\",Wp.shape)\n\n        #apply weighting = 4\n        Mpersona = (Wp > self.tau).float()\n        print(\"Mpersona:\",Mpersona.shape)\n        Mcontext = (1 - Wp > self.tau).float()\n        print(\"Mcontext:\",Mcontext.shape)\n\n        #apply masking to the cross attention / element wise multiplication = 5\n        oP_weighted = Mpersona * oP\n        print(\"oP_weighted:\",oP_weighted.shape)\n        oC_weighted = Mcontext * oC\n        print(\"oC_weighted:\",oC_weighted.shape)\n\n        #adding the cross attention results = 6\n        HPAA = oP_weighted + oC_weighted #output resulted from PAA layers\n        print(\"HPAA:\",HPAA.shape)\n        return HPAA     \n        \n\n        \n\n        ","metadata":{"_uuid":"38e22573-3027-4a51-98b8-e533d286fc0d","_cell_guid":"98759362-3917-4709-afe7-cebc9c9a4513","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-18T12:02:16.131863Z","iopub.execute_input":"2024-12-18T12:02:16.132576Z","iopub.status.idle":"2024-12-18T12:02:16.139489Z","shell.execute_reply.started":"2024-12-18T12:02:16.132541Z","shell.execute_reply":"2024-12-18T12:02:16.138605Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# PAA Model","metadata":{}},{"cell_type":"markdown","source":"* metacognitive_response: Tensor of shape (batch_size, seq_len, hidden_size)\n* metacognitive_vector: Tensor of shape (batch_size, hidden_size)\n* encoded_context: Tensor of shape (batch_size, context_len, hidden_size)\n* encoded_persona: Tensor of shape (batch_size, persona_len, hidden_size)","metadata":{}},{"cell_type":"code","source":"class PAAModel(nn.Module):\n    def __init__(self, hidden_size=768, vocab_size = 30522 ,tau=0.5, max_length=512):\n        super(PAAModel , self).__init__()\n        self.hidden_size = hidden_size\n        self.tau = tau\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        #self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.add_special_tokens({'pad_token': '[BOS]'})\n        self.embedding = nn.Embedding(len(self.tokenizer), self.hidden_size)\n        \n\n        #transformer decoder to decode the metacognitive response\n        #self.transformer_decoder = nn.TransformerDecoderLayer(d_model = hidden_size , nhead=8)\n        #self.decoder = nn.TransformerDecoder(self.transformer_decoder, num_layers=6)\n        self.decoder = GPT2LMHeadModel.from_pretrained('gpt2')\n        \n\n\n        # Attention layers for persona and context\n        self.context_attn = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.persona_attn = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n\n        \n        #linear FC output layer\n        self.fc_out = nn.Linear(hidden_size, vocab_size)\n        #PAA layer\n        self.paa = PAALayer(hidden_dimension=hidden_size, tau=tau)\n\n    def forward(self, metacognitive_response , metacognitive_vector, encoded_context , encoded_persona):\n        \n        tokenized_response = self.tokenizer(\n            metacognitive_response, \n            return_tensors='pt', \n            padding='max_length', \n            truncation=True, \n            max_length=self.max_length-16\n        )['input_ids']\n        \n        print(\"tokenized response:\" ,tokenized_response.shape)\n        print(\"tokenizer:\" , self.tokenizer)        \n        \n        metacognitive_response_emb = self.embedding(tokenized_response)\n        print(\"metacognitive response embedding shape:\" , metacognitive_response_emb.shape)\n\n        #print(\"metacognitive vector long:\", metacognitive_vector.shape)\n        # metacognitive_vector_ids = torch.tensor(metacognitive_vector)\n        #print(\"metacognitive vector token ids shape:\", tokenized_vector.shape)\n        # metacognitive_vector_ids = metacognitive_vector_ids.expand(1, tokenized_response.shape[1])  # Adjust to the length of tokenized response\n        metacognitive_vector_emb = self.embedding(metacognitive_vector)\n        print(\"metacognitive vector emb shape:\" , metacognitive_vector_emb.shape)\n        # print(\"metacognitive vector token ids shape:\", metacognitive_vector_ids.shape)\n        \n        #deocder input - concatenate reposnse + vector = step 1\n        decoder_input = torch.cat([metacognitive_response_emb, metacognitive_vector_emb], dim=1)\n        print(\"decoder input shape:\" ,decoder_input.shape)\n\n        #step 2 - self attention on input embedding\n        # Use a dummy memory tensor filled with zeros if no encoder memory is provided\n        memory = torch.zeros((decoder_input.size(0), 1, self.hidden_size)).to(decoder_input.device)\n        inputs_embeds = decoder_input\n        outputs = self.decoder(inputs_embeds=inputs_embeds, output_hidden_states=True)\n        hR = (outputs.hidden_states)[-1]\n        #hR = self.decoder(decoder_input , memory)\n        print(\"hR shape:\" , hR.shape)\n\n        #step 3 = cross attention\n        oP, _ = self.persona_attn(hR, encoded_persona, encoded_persona)\n        oC, _ = self.context_attn(hR, encoded_context, encoded_context)\n        print(\"oP shape:\" , oP.shape)\n        print(\"oC shape:\" , oC.shape)\n\n\n        #step 4 = apply PAA layer\n        HPAA = self.paa(hR , oP, oC)\n        print(\"HPAA shape:\"  , HPAA.shape)\n\n        #step 5 = linear output\n        logits = self.fc_out(HPAA)\n        print(\"logits shape:\" , logits.shape)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:54:36.754316Z","iopub.execute_input":"2024-12-18T12:54:36.755093Z","iopub.status.idle":"2024-12-18T12:54:36.765293Z","shell.execute_reply.started":"2024-12-18T12:54:36.755059Z","shell.execute_reply":"2024-12-18T12:54:36.764369Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"model = PAAModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:54:56.406008Z","iopub.execute_input":"2024-12-18T12:54:56.406598Z","iopub.status.idle":"2024-12-18T12:54:58.561137Z","shell.execute_reply.started":"2024-12-18T12:54:56.406565Z","shell.execute_reply":"2024-12-18T12:54:58.560445Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:02:36.837746Z","iopub.execute_input":"2024-12-18T12:02:36.838460Z","iopub.status.idle":"2024-12-18T12:02:36.844384Z","shell.execute_reply.started":"2024-12-18T12:02:36.838404Z","shell.execute_reply":"2024-12-18T12:02:36.843554Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"PAAModel(\n  (embedding): Embedding(50258, 768)\n  (decoder): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2SdpaAttention(\n            (c_attn): Conv1D(nf=2304, nx=768)\n            (c_proj): Conv1D(nf=768, nx=768)\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D(nf=3072, nx=768)\n            (c_proj): Conv1D(nf=768, nx=3072)\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  )\n  (context_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (persona_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (fc_out): Linear(in_features=768, out_features=30522, bias=True)\n  (paa): PAALayer(\n    (fc): Linear(in_features=1536, out_features=768, bias=True)\n    (sigmoid): Sigmoid()\n  )\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:02:39.945142Z","iopub.execute_input":"2024-12-18T12:02:39.945957Z","iopub.status.idle":"2024-12-18T12:02:39.950819Z","shell.execute_reply.started":"2024-12-18T12:02:39.945921Z","shell.execute_reply":"2024-12-18T12:02:39.949972Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:04:01.837957Z","iopub.execute_input":"2024-12-18T12:04:01.838726Z","iopub.status.idle":"2024-12-18T12:04:02.159965Z","shell.execute_reply.started":"2024-12-18T12:04:01.838690Z","shell.execute_reply":"2024-12-18T12:04:02.159265Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"LOSS = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:23:38.346360Z","iopub.execute_input":"2024-12-18T12:23:38.347249Z","iopub.status.idle":"2024-12-18T12:23:38.351177Z","shell.execute_reply.started":"2024-12-18T12:23:38.347214Z","shell.execute_reply":"2024-12-18T12:23:38.350200Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"num_epochs = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:04:28.675693Z","iopub.execute_input":"2024-12-18T12:04:28.676366Z","iopub.status.idle":"2024-12-18T12:04:28.679903Z","shell.execute_reply.started":"2024-12-18T12:04:28.676337Z","shell.execute_reply":"2024-12-18T12:04:28.679004Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import ast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:04:30.879059Z","iopub.execute_input":"2024-12-18T12:04:30.879694Z","iopub.status.idle":"2024-12-18T12:04:30.883400Z","shell.execute_reply.started":"2024-12-18T12:04:30.879656Z","shell.execute_reply":"2024-12-18T12:04:30.882416Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:30:53.885735Z","iopub.execute_input":"2024-12-18T12:30:53.886077Z","iopub.status.idle":"2024-12-18T12:30:53.890734Z","shell.execute_reply.started":"2024-12-18T12:30:53.886048Z","shell.execute_reply":"2024-12-18T12:30:53.889723Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:30:55.958738Z","iopub.execute_input":"2024-12-18T12:30:55.959313Z","iopub.status.idle":"2024-12-18T12:30:55.964950Z","shell.execute_reply.started":"2024-12-18T12:30:55.959281Z","shell.execute_reply":"2024-12-18T12:30:55.964133Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t50257: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:12:43.178874Z","iopub.execute_input":"2024-12-18T12:12:43.179238Z","iopub.status.idle":"2024-12-18T12:12:43.184890Z","shell.execute_reply.started":"2024-12-18T12:12:43.179207Z","shell.execute_reply":"2024-12-18T12:12:43.184030Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"50257"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"model.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:12:52.222400Z","iopub.execute_input":"2024-12-18T12:12:52.222777Z","iopub.status.idle":"2024-12-18T12:12:52.229906Z","shell.execute_reply.started":"2024-12-18T12:12:52.222746Z","shell.execute_reply":"2024-12-18T12:12:52.229086Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"30522"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for idx,row in df.iterrows():\n        description = row['description']\n        student_code = row['student_code']\n        feedback = row['feedback']\n        metacognitive_feedback = row['metacognitive_feedback']\n        metacognitive_vector= row['metacognitive_profile']\n        print(\"metacognitive_vector:\" ,metacognitive_vector)\n\n        metacognitive_tensor = torch.tensor([\n            ast.literal_eval(metacognitive_vector) ], dtype=torch.float)\n        \n        print(\"metacognitive vector shape:\", metacognitive_tensor.shape)\n        print(metacognitive_tensor)\n        print(\"metacognitive vector shape unsqueeze:\", metacognitive_tensor.unsqueeze(1).shape)\n        \n        \n        #metacognitive_feedback = ast.literal_eval(metacognitive_feedback)\n        #metacognitive_feedback_tensor = torch.tensor(metacognitive_feedback, dtype=torch.float32).unsqueeze(0) \n        \n\n        context_encoding = context_encoder(description, student_code , feedback)\n        persona_encoding = persona_encoder(metacognitive_tensor)\n        print(\"encoded context:\",context_encoding.shape)\n        print(\"encoded persona:\",persona_encoding.shape)\n\n        \n\n        logits = model(metacognitive_feedback, metacognitive_tensor.long(), context_encoding, persona_encoding)\n        print(\"logits.shape\" ,logits.shape)\n\n        logits = logits.view(-1, logits.size(-1)) \n        target_ids = target_labels.view(-1)\n        \n        print(\"target labels:\" , (logits.shape, target_ids.shape))\n\n        #print(\"Target label values:\", target_labels)\n        print(\"Maximum target value:\", target_labels.max())\n        print(\"Model vocab size:\", model.vocab_size)\n        print(\"------------------------------------------------\")\n        print(\"target:\",target_ids)\n        print(\"------------------------------------------------\")\n        print(\"logits:\",logits)\n        loss = LOSS(logits,target_ids)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(df):.4f}\")\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T12:55:01.962843Z","iopub.execute_input":"2024-12-18T12:55:01.963682Z","iopub.status.idle":"2024-12-18T12:55:03.975288Z","shell.execute_reply.started":"2024-12-18T12:55:01.963646Z","shell.execute_reply":"2024-12-18T12:55:03.973891Z"}},"outputs":[{"name":"stdout","text":"metacognitive_vector: [2, 1, 3, 3, 2, 3, 2, 1, 3, 1, 1, 3, 2, 1, 2, 1]\nmetacognitive vector shape: torch.Size([1, 16])\ntensor([[2., 1., 3., 3., 2., 3., 2., 1., 3., 1., 1., 3., 2., 1., 2., 1.]])\nmetacognitive vector shape unsqueeze: torch.Size([1, 1, 16])\nencoded context: torch.Size([1, 1, 768])\nencoded persona: torch.Size([1, 1, 768])\ntokenized response: torch.Size([1, 496])\ntokenizer: GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '[BOS]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t50257: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\nmetacognitive response embedding shape: torch.Size([1, 496, 768])\nmetacognitive vector emb shape: torch.Size([1, 16, 768])\ndecoder input shape: torch.Size([1, 512, 768])\nhR shape: torch.Size([1, 512, 768])\noP shape: torch.Size([1, 512, 768])\noC shape: torch.Size([1, 512, 768])\nMp input: torch.Size([1, 512, 1536])\nMp: torch.Size([1, 512, 768])\nWp: torch.Size([1, 512, 768])\nMpersona: torch.Size([1, 512, 768])\nMcontext: torch.Size([1, 512, 768])\noP_weighted: torch.Size([1, 512, 768])\noC_weighted: torch.Size([1, 512, 768])\nHPAA: torch.Size([1, 512, 768])\nHPAA shape: torch.Size([1, 512, 768])\nlogits shape: torch.Size([1, 512, 30522])\nlogits.shape torch.Size([1, 512, 30522])\ntarget labels: (torch.Size([512, 30522]), torch.Size([512]))\nMaximum target value: tensor(50256)\nModel vocab size: 30522\n------------------------------------------------\ntarget: tensor([ 1026,  3568,   326,   345,   389,  2048,   319,   262,   826,  2610,\n          351,   534,  3164,   284, 18120,   262,  1917,    11,   475,   612,\n          389,   257,  1178,  3006,   810,   345,  1244,   761,   284,  4532,\n          534, 10064,   284,  2987,   534,  9922,   290,  4547,    13,  3914,\n          338,  2270,   866,   262,  4831,   284,  1037,   345,  9494,   534,\n         1917,    12,    82, 10890,  1429,    13,   198,   198,  5962,    11,\n          345,  4750,   326,   345,  1690,  1100,   262,  1808,  5000,   878,\n         3599,   357, 24361,   352,   828,   543,   318,  1049,    13,  2102,\n           11,   340,   338,  8780,   284,   635,   302, 34675,   262,  1808,\n          287,   534,   898,  2456,   284,  4155,   345,  3938,  1833,   262,\n         5359,   357, 24361,   513,   737,   770,   481,  1037,   345,  5911,\n         1994,  3307,   588,   262,   761,   284,  6152,   262,  2836,   351,\n          257,  2176,  3275,   290,  3650,   262,  2457,  3280,   287,   257,\n         7885,  3706,   705, 20274,  4458,   198,   198,  1639,  2098,   326,\n          345,  3360,  2270,   866,   262,  1917,   656,  4833,   850,    12,\n         2188,   874,   357, 24361,   642,   737,  1114,   428,  4876,    11,\n         2270,   340,   866,   656,  1115,  1388,  4831,    25, 21550,   262,\n         2836,   329,  5128,    11, 26019,   262,  5951, 11315,    11,   290,\n        13570,   262,  1255,    13,   770,   481,   787,   262,  1917,   517,\n        36426,    13,   198,   198,  2215, 15427,   534,  4610,    11,   345,\n         4750,   326,   345,  1690, 17548,   503,   262, 11862,   878, 19617,\n          357, 24361,   767,   737,   770,   318,   257,   922,  3357,    11,\n          475,  3505,   284,   635,   307, 30257,  1141,   262,  7822,  1429,\n          284,  3368, 10135,   357, 24361,   860,   737,  1114,  4554,    11,\n         4155,   326,   534,  5128,  6152,   318,  9380, 39559,   355,   257,\n         4731,    11,   543,   318,   257,  2219, 15662,    13,   198,   198,\n         1639,   635,  4750,   326,   345,  3360,  5671,   262,  7044,  7822,\n         1429,   357, 24361,  1105,   737,  1675,  2987,   428,    11,  2074,\n        10627, 19898,  2482,   284, 11767,   326,  1123,  2239,   318,  3376,\n           13,  1114,  1672,    11,   706, 23202,   262,  5951,    11,  3601,\n          262,  1255,   284,  4155,   340,   338,  7187,   878, 18788,    13,\n          198,   198, 11158,    11,   345,  2098,   326,   345,  3360,  6216,\n          611,   262,  2457,  7822,   318,  3376,   357, 24361,  1478,   737,\n         1675,  9494,   428,    11,  3522,   736,   284,   262,  1917,  2643,\n          290,  4155,   326,   534,  4610, 11185,   477,   262,  5359,    11,\n          884,   355, 23069,   262,  1255,   287,   257,  7885,  3706,   705,\n        20274,     6,   290, 13570,   262,  5072,  9380,    13,   198,   198,\n         3886, 29927,   777, 10064,   517,  9835,    11,   345,   460,  2987,\n          534,  1917,    12,    82, 10890,  4678,   290,  4155,   326,   534,\n         8136,   389,  7187,   290,  1844,    13,  9175,   510,   262,   922,\n          670,    11,   290,  3505,   284,  4174,   777,  1138,   330, 46610,\n        10064,   284,  2003,  2761,   355,   880,    13, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n        50256, 50256])\n------------------------------------------------\nlogits: tensor([[-0.0370, -0.2608, -0.0632,  ...,  0.0950, -0.0407,  0.1334],\n        [-0.0055, -0.2065, -0.0925,  ...,  0.1458, -0.0302,  0.1163],\n        [-0.0350, -0.2740, -0.0554,  ...,  0.1101,  0.0269,  0.0714],\n        ...,\n        [-0.0197, -0.2168, -0.1403,  ...,  0.1090, -0.0430,  0.1358],\n        [-0.0421, -0.1948, -0.1852,  ...,  0.0792, -0.0974,  0.0906],\n        [ 0.0284, -0.2136, -0.1400,  ...,  0.1665, -0.1176,  0.0973]],\n       grad_fn=<ViewBackward0>)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[90], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits:\u001b[39m\u001b[38;5;124m\"\u001b[39m,logits)\n\u001b[0;32m---> 47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mLOSS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     50\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mIndexError\u001b[0m: Target 34675 is out of bounds."],"ename":"IndexError","evalue":"Target 34675 is out of bounds.","output_type":"error"}],"execution_count":90},{"cell_type":"markdown","source":"# Inferencing","metadata":{}},{"cell_type":"code","source":"def inference(model, context_encoder, persona_encoder, description, student_code, feedback, metacognitive_feedback):\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():  # Disable gradient computation for inference\n        # Step 1: Convert the input data to the necessary tensors\n        metacognitive_tensor = torch.tensor(\n            [ast.literal_eval(metacognitive_feedback)], dtype=torch.float\n        )\n\n        # Step 2: Encode context and persona using the respective encoders\n        context_encoding = context_encoder(description, student_code, feedback)  # Shape: [1, 768]\n        persona_encoding = persona_encoder(metacognitive_tensor)  # Shape: [1, 768]\n\n        # Step 3: Prepare the metacognitive response (this is the input for the decoder)\n        metacognitive_response = torch.tensor(\n            [ast.literal_eval(metacognitive_feedback)], dtype=torch.float\n        )  # Assuming metacognitive_feedback is in the required format\n\n        # Step 4: Pass everything through the model (no gradients needed for inference)\n        logits = model(metacognitive_response, metacognitive_tensor.long(), context_encoding, persona_encoding)\n\n        # Step 5: Convert logits to predictions (for simplicity, let's take the argmax here)\n        predictions = torch.argmax(logits, dim=-1)  # Taking argmax along the vocabulary dimension\n\n        # Step 6: If needed, convert the predicted tokens back to text (this step depends on your output format)\n        predicted_tokens = predictions.squeeze().tolist()\n\n        # Convert tokens to text (if using a tokenizer for language generation, replace this step)\n        # For simplicity, assuming the output is a sequence of tokens\n        return predicted_tokens\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:11:00.316482Z","iopub.execute_input":"2024-12-18T09:11:00.317240Z","iopub.status.idle":"2024-12-18T09:11:00.323232Z","shell.execute_reply.started":"2024-12-18T09:11:00.317205Z","shell.execute_reply":"2024-12-18T09:11:00.322392Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"row = df.iloc[0]  \n\ndescription = row['description']\nstudent_code = row['student_code']\nfeedback = row['feedback']\nmetacognitive_feedback = row['metacognitive_feedback']\nmetacognitive_vector= row['metacognitive_profile']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_tokens = inference(model, context_encoder, persona_encoder, description, student_code, feedback, metacognitive_feedback)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Predicted tokens: {predicted_tokens}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.encode(\"Hello, my dog is cute\", \n                 add_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}